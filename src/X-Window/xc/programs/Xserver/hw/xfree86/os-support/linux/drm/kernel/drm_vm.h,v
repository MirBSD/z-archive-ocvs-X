head	1.1;
branch	1.1.1;
access;
symbols
	tg-mergetmp-2:1.1.1.3
	cvs-200410241530:1.1.1.3
	cvs-200410012000:1.1.1.3
	cvs-200407221130:1.1.1.3
	cvs-200407141120:1.1.1.3
	cvs-200406231010:1.1.1.3
	MIRBSD_7quater:1.1.1.2
	cvs-200405271510:1.1.1.3
	XFree86_4_4_0:1.1.9.1
	cvs-200403021700:1.1.1.3
	XFREE86_20040213:1.1.9.1
	xc:1.1.9
	cvs-200401291925:1.1.1.2
	MIRBSD_7_ALPHA:1.1.1.2.0.4
	MIRBSD_7:1.1.1.2.0.2
	MIRBSD_7ter:1.1.1.2
	cvs-20011091815:1.1.1.2
	cvs-200309162130:1.1.1.2
	cvs-200308302005:1.1.1.2
	ctmx-0387:1.1.1.2
	ctmx-0384:1.1.1.2
	MIRBSD_5:1.1.1.2
	ctmx-0375:1.1.1.2
	ctmx-0373:1.1.1.2
	ctm-0371:1.1.1.2
	ctm-0370:1.1.1.2
	MIRBSD_4:1.1.1.2
	ctm-0363:1.1.1.2
	ctm-0359:1.1.1.2
	ctm-0349:1.1.1.1
	openbsd:1.1.1;
locks; strict;
comment	@ * @;


1.1
date	2003.03.22.20.08.44;	author tg;	state Exp;
branches
	1.1.1.1
	1.1.9.1;
next	;

1.1.1.1
date	2003.03.22.20.08.44;	author tg;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2003.04.08.18.37.35;	author tg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2004.03.02.18.25.18;	author tg;	state Stab;
branches;
next	;

1.1.9.1
date	2004.02.14.19.25.14;	author tg;	state Exp;
branches;
next	;


desc
@@


1.1
log
@Initial revision
@
text
@/* drm_vm.h -- Memory mapping for DRM -*- linux-c -*-
 * Created: Mon Jan  4 08:58:31 1999 by faith@@valinux.com
 *
 * Copyright 1999 Precision Insight, Inc., Cedar Park, Texas.
 * Copyright 2000 VA Linux Systems, Inc., Sunnyvale, California.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 *
 * Authors:
 *    Rickard E. (Rik) Faith <faith@@valinux.com>
 *    Gareth Hughes <gareth@@valinux.com>
 */

#define __NO_VERSION__
#include "drmP.h"

struct vm_operations_struct   DRM(vm_ops) = {
	nopage:	 DRM(vm_nopage),
	open:	 DRM(vm_open),
	close:	 DRM(vm_close),
};

struct vm_operations_struct   DRM(vm_shm_ops) = {
	nopage:	 DRM(vm_shm_nopage),
	open:	 DRM(vm_open),
	close:	 DRM(vm_shm_close),
};

struct vm_operations_struct   DRM(vm_dma_ops) = {
	nopage:	 DRM(vm_dma_nopage),
	open:	 DRM(vm_open),
	close:	 DRM(vm_close),
};

struct vm_operations_struct   DRM(vm_sg_ops) = {
	nopage:  DRM(vm_sg_nopage),
	open:    DRM(vm_open),
	close:   DRM(vm_close),
};

struct page *DRM(vm_nopage)(struct vm_area_struct *vma,
			    unsigned long address,
			    int write_access)
{
#if __REALLY_HAVE_AGP
	drm_file_t *priv  = vma->vm_file->private_data;
	drm_device_t *dev = priv->dev;
	drm_map_t *map    = NULL;
	drm_map_list_t  *r_list;
	struct list_head *list;

	/*
         * Find the right map
         */

	if(!dev->agp->cant_use_aperture) goto vm_nopage_error;

	list_for_each(list, &dev->maplist->head) {
		r_list = (drm_map_list_t *)list;
		map = r_list->map;
		if (!map) continue;
		if (map->offset == VM_OFFSET(vma)) break;
	}

	if (map && map->type == _DRM_AGP) {
		unsigned long offset = address - vma->vm_start;
		unsigned long baddr = VM_OFFSET(vma) + offset;
		struct drm_agp_mem *agpmem;
		struct page *page;

#if __alpha__
		/*
                 * Adjust to a bus-relative address
                 */
		baddr -= dev->hose->mem_space->start;
#endif

		/*
                 * It's AGP memory - find the real physical page to map
                 */
		for(agpmem = dev->agp->memory; agpmem; agpmem = agpmem->next) {
			if (agpmem->bound <= baddr &&
			    agpmem->bound + agpmem->pages * PAGE_SIZE > baddr) 
				break;
		}

		if (!agpmem) goto vm_nopage_error;

		/*
                 * Get the page, inc the use count, and return it
                 */
		offset = (baddr - agpmem->bound) >> PAGE_SHIFT;
		agpmem->memory->memory[offset] &= dev->agp->page_mask;
		page = virt_to_page(__va(agpmem->memory->memory[offset]));
		get_page(page);

		DRM_DEBUG("baddr = 0x%lx page = 0x%p, offset = 0x%lx\n",
			  baddr, __va(agpmem->memory->memory[offset]), offset);

		return page;
        }
vm_nopage_error:
#endif /* __REALLY_HAVE_AGP */

	return NOPAGE_SIGBUS;		/* Disallow mremap */
}

struct page *DRM(vm_shm_nopage)(struct vm_area_struct *vma,
				unsigned long address,
				int write_access)
{
	drm_map_t	 *map	 = (drm_map_t *)vma->vm_private_data;
	unsigned long	 offset;
	unsigned long	 i;
	pgd_t		 *pgd;
	pmd_t		 *pmd;
	pte_t		 *pte;
	struct page	 *page;

	if (address > vma->vm_end) return NOPAGE_SIGBUS; /* Disallow mremap */
	if (!map)    		   return NOPAGE_OOM;  /* Nothing allocated */

	offset	 = address - vma->vm_start;
	i = (unsigned long)map->handle + offset;
	/* We have to walk page tables here because we need large SAREA's, and
	 * they need to be virtually contiguous in kernel space.
	 */
	pgd = pgd_offset_k( i );
	if( !pgd_present( *pgd ) ) return NOPAGE_OOM;
	pmd = pmd_offset( pgd, i );
	if( !pmd_present( *pmd ) ) return NOPAGE_OOM;
	pte = pte_offset( pmd, i );
	if( !pte_present( *pte ) ) return NOPAGE_OOM;

	page = pte_page(*pte);
	get_page(page);

	DRM_DEBUG("0x%08lx => 0x%08x\n", address, page_to_bus(page));
	return page;
}

/* Special close routine which deletes map information if we are the last
 * person to close a mapping and its not in the global maplist.
 */

void DRM(vm_shm_close)(struct vm_area_struct *vma)
{
	drm_file_t	*priv	= vma->vm_file->private_data;
	drm_device_t	*dev	= priv->dev;
	drm_vma_entry_t *pt, *prev, *next;
	drm_map_t *map;
	drm_map_list_t *r_list;
	struct list_head *list;
	int found_maps = 0;

	DRM_DEBUG("0x%08lx,0x%08lx\n",
		  vma->vm_start, vma->vm_end - vma->vm_start);
	atomic_dec(&dev->vma_count);

	map = vma->vm_private_data;

	down(&dev->struct_sem);
	for (pt = dev->vmalist, prev = NULL; pt; pt = next) {
		next = pt->next;
		if (pt->vma->vm_private_data == map) found_maps++;
		if (pt->vma == vma) {
			if (prev) {
				prev->next = pt->next;
			} else {
				dev->vmalist = pt->next;
			}
			DRM(free)(pt, sizeof(*pt), DRM_MEM_VMAS);
		} else {
			prev = pt;
		}
	}
	/* We were the only map that was found */
	if(found_maps == 1 &&
	   map->flags & _DRM_REMOVABLE) {
		/* Check to see if we are in the maplist, if we are not, then
		 * we delete this mappings information.
		 */
		found_maps = 0;
		list = &dev->maplist->head;
		list_for_each(list, &dev->maplist->head) {
			r_list = (drm_map_list_t *) list;
			if (r_list->map == map) found_maps++;
		}

		if(!found_maps) {
			switch (map->type) {
			case _DRM_REGISTERS:
			case _DRM_FRAME_BUFFER:
#if __REALLY_HAVE_MTRR
				if (map->mtrr >= 0) {
					int retcode;
					retcode = mtrr_del(map->mtrr,
							   map->offset,
							   map->size);
					DRM_DEBUG("mtrr_del = %d\n", retcode);
				}
#endif
				DRM(ioremapfree)(map->handle, map->size);
				break;
			case _DRM_SHM:
				vfree(map->handle);
				break;
			case _DRM_AGP:
			case _DRM_SCATTER_GATHER:
				break;
			}
			DRM(free)(map, sizeof(*map), DRM_MEM_MAPS);
		}
	}
	up(&dev->struct_sem);
}

struct page *DRM(vm_dma_nopage)(struct vm_area_struct *vma,
				unsigned long address,
				int write_access)
{
	drm_file_t	 *priv	 = vma->vm_file->private_data;
	drm_device_t	 *dev	 = priv->dev;
	drm_device_dma_t *dma	 = dev->dma;
	unsigned long	 offset;
	unsigned long	 page_nr;
	struct page	 *page;

	if (!dma)		   return NOPAGE_SIGBUS; /* Error */
	if (address > vma->vm_end) return NOPAGE_SIGBUS; /* Disallow mremap */
	if (!dma->pagelist)	   return NOPAGE_OOM ; /* Nothing allocated */

	offset	 = address - vma->vm_start; /* vm_[pg]off[set] should be 0 */
	page_nr  = offset >> PAGE_SHIFT;
	page = virt_to_page((dma->pagelist[page_nr] + 
			     (offset & (~PAGE_MASK))));

	get_page(page);

	DRM_DEBUG("0x%08lx (page %lu) => 0x%08x\n", address, page_nr, 
		  page_to_bus(page));
	return page;
}

struct page *DRM(vm_sg_nopage)(struct vm_area_struct *vma,
			       unsigned long address,
			       int write_access)
{
	drm_map_t        *map    = (drm_map_t *)vma->vm_private_data;
	drm_file_t *priv = vma->vm_file->private_data;
	drm_device_t *dev = priv->dev;
	drm_sg_mem_t *entry = dev->sg;
	unsigned long offset;
	unsigned long map_offset;
	unsigned long page_offset;
	struct page *page;

	if (!entry)                return NOPAGE_SIGBUS; /* Error */
	if (address > vma->vm_end) return NOPAGE_SIGBUS; /* Disallow mremap */
	if (!entry->pagelist)      return NOPAGE_OOM ;  /* Nothing allocated */


	offset = address - vma->vm_start;
	map_offset = map->offset - dev->sg->handle;
	page_offset = (offset >> PAGE_SHIFT) + (map_offset >> PAGE_SHIFT);
	page = entry->pagelist[page_offset];
	get_page(page);

	return page;
}

void DRM(vm_open)(struct vm_area_struct *vma)
{
	drm_file_t	*priv	= vma->vm_file->private_data;
	drm_device_t	*dev	= priv->dev;
	drm_vma_entry_t *vma_entry;

	DRM_DEBUG("0x%08lx,0x%08lx\n",
		  vma->vm_start, vma->vm_end - vma->vm_start);
	atomic_inc(&dev->vma_count);

	vma_entry = DRM(alloc)(sizeof(*vma_entry), DRM_MEM_VMAS);
	if (vma_entry) {
		down(&dev->struct_sem);
		vma_entry->vma	= vma;
		vma_entry->next = dev->vmalist;
		vma_entry->pid	= current->pid;
		dev->vmalist	= vma_entry;
		up(&dev->struct_sem);
	}
}

void DRM(vm_close)(struct vm_area_struct *vma)
{
	drm_file_t	*priv	= vma->vm_file->private_data;
	drm_device_t	*dev	= priv->dev;
	drm_vma_entry_t *pt, *prev;

	DRM_DEBUG("0x%08lx,0x%08lx\n",
		  vma->vm_start, vma->vm_end - vma->vm_start);
	atomic_dec(&dev->vma_count);

	down(&dev->struct_sem);
	for (pt = dev->vmalist, prev = NULL; pt; prev = pt, pt = pt->next) {
		if (pt->vma == vma) {
			if (prev) {
				prev->next = pt->next;
			} else {
				dev->vmalist = pt->next;
			}
			DRM(free)(pt, sizeof(*pt), DRM_MEM_VMAS);
			break;
		}
	}
	up(&dev->struct_sem);
}

int DRM(mmap_dma)(struct file *filp, struct vm_area_struct *vma)
{
	drm_file_t	 *priv	 = filp->private_data;
	drm_device_t	 *dev;
	drm_device_dma_t *dma;
	unsigned long	 length	 = vma->vm_end - vma->vm_start;

	lock_kernel();
	dev	 = priv->dev;
	dma	 = dev->dma;
	DRM_DEBUG("start = 0x%lx, end = 0x%lx, offset = 0x%lx\n",
		  vma->vm_start, vma->vm_end, VM_OFFSET(vma));

				/* Length must match exact page count */
	if (!dma || (length >> PAGE_SHIFT) != dma->page_count) {
		unlock_kernel();
		return -EINVAL;
	}
	unlock_kernel();

	vma->vm_ops   = &DRM(vm_dma_ops);

#if LINUX_VERSION_CODE <= 0x020414
	vma->vm_flags |= VM_LOCKED | VM_SHM; /* Don't swap */
#else
	vma->vm_flags |= VM_RESERVED; /* Don't swap */
#endif

	vma->vm_file  =	 filp;	/* Needed for drm_vm_open() */
	DRM(vm_open)(vma);
	return 0;
}

#ifndef DRIVER_GET_MAP_OFS
#define DRIVER_GET_MAP_OFS()	(map->offset)
#endif

#ifndef DRIVER_GET_REG_OFS
#ifdef __alpha__
#define DRIVER_GET_REG_OFS()	(dev->hose->dense_mem_base -	\
				 dev->hose->mem_space->start)
#else
#define DRIVER_GET_REG_OFS()	0
#endif
#endif

int DRM(mmap)(struct file *filp, struct vm_area_struct *vma)
{
	drm_file_t	*priv	= filp->private_data;
	drm_device_t	*dev	= priv->dev;
	drm_map_t	*map	= NULL;
	drm_map_list_t  *r_list;
	unsigned long   offset  = 0;
	struct list_head *list;

	DRM_DEBUG("start = 0x%lx, end = 0x%lx, offset = 0x%lx\n",
		  vma->vm_start, vma->vm_end, VM_OFFSET(vma));

	if ( !priv->authenticated ) return -EACCES;

	if (!VM_OFFSET(vma)) return DRM(mmap_dma)(filp, vma);

				/* A sequential search of a linked list is
				   fine here because: 1) there will only be
				   about 5-10 entries in the list and, 2) a
				   DRI client only has to do this mapping
				   once, so it doesn't have to be optimized
				   for performance, even if the list was a
				   bit longer. */
	list_for_each(list, &dev->maplist->head) {
		unsigned long off;

		r_list = (drm_map_list_t *)list;
		map = r_list->map;
		if (!map) continue;
		off = DRIVER_GET_MAP_OFS();
		if (off == VM_OFFSET(vma)) break;
	}

	if (!map || ((map->flags&_DRM_RESTRICTED) && !capable(CAP_SYS_ADMIN)))
		return -EPERM;

				/* Check for valid size. */
	if (map->size != vma->vm_end - vma->vm_start) return -EINVAL;

	if (!capable(CAP_SYS_ADMIN) && (map->flags & _DRM_READ_ONLY)) {
		vma->vm_flags &= VM_MAYWRITE;
#if defined(__i386__)
		pgprot_val(vma->vm_page_prot) &= ~_PAGE_RW;
#else
				/* Ye gads this is ugly.  With more thought
                                   we could move this up higher and use
                                   `protection_map' instead.  */
		vma->vm_page_prot = __pgprot(pte_val(pte_wrprotect(
			__pte(pgprot_val(vma->vm_page_prot)))));
#endif
	}

	switch (map->type) {
        case _DRM_AGP:
#if defined(__alpha__)
                /*
                 * On Alpha we can't talk to bus dma address from the
                 * CPU, so for memory of type DRM_AGP, we'll deal with
                 * sorting out the real physical pages and mappings
                 * in nopage()
                 */
                vma->vm_ops = &DRM(vm_ops);
                break;
#endif
                /* fall through to _DRM_FRAME_BUFFER... */        
	case _DRM_FRAME_BUFFER:
	case _DRM_REGISTERS:
		if (VM_OFFSET(vma) >= __pa(high_memory)) {
#if defined(__i386__)
			if (boot_cpu_data.x86 > 3 && map->type != _DRM_AGP) {
				pgprot_val(vma->vm_page_prot) |= _PAGE_PCD;
				pgprot_val(vma->vm_page_prot) &= ~_PAGE_PWT;
			}
#elif defined(__ia64__)
			if (map->type != _DRM_AGP)
				vma->vm_page_prot =
					pgprot_writecombine(vma->vm_page_prot);
#elif defined(__powerpc__)
			pgprot_val(vma->vm_page_prot) |= _PAGE_NO_CACHE | _PAGE_GUARDED;
#endif
			vma->vm_flags |= VM_IO;	/* not in core dump */
		}
		offset = DRIVER_GET_REG_OFS();
		if (remap_page_range(vma->vm_start,
				     VM_OFFSET(vma) + offset,
				     vma->vm_end - vma->vm_start,
				     vma->vm_page_prot))
				return -EAGAIN;
		DRM_DEBUG("   Type = %d; start = 0x%lx, end = 0x%lx,"
			  " offset = 0x%lx\n",
			  map->type,
			  vma->vm_start, vma->vm_end, VM_OFFSET(vma) + offset);
		vma->vm_ops = &DRM(vm_ops);
		break;
	case _DRM_SHM:
		vma->vm_ops = &DRM(vm_shm_ops);
		vma->vm_private_data = (void *)map;
				/* Don't let this area swap.  Change when
				   DRM_KERNEL advisory is supported. */
#if LINUX_VERSION_CODE <= 0x020414
		vma->vm_flags |= VM_LOCKED;
#else
		vma->vm_flags |= VM_RESERVED;
#endif
		break;
	case _DRM_SCATTER_GATHER:
		vma->vm_ops = &DRM(vm_sg_ops);
		vma->vm_private_data = (void *)map;
#if LINUX_VERSION_CODE <= 0x020414
		vma->vm_flags |= VM_LOCKED;
#else
		vma->vm_flags |= VM_RESERVED;
#endif
                break;
	default:
		return -EINVAL;	/* This should never happen. */
	}
#if LINUX_VERSION_CODE <= 0x020414
	vma->vm_flags |= VM_LOCKED | VM_SHM; /* Don't swap */
#else
	vma->vm_flags |= VM_RESERVED; /* Don't swap */
#endif

	vma->vm_file  =	 filp;	/* Needed for drm_vm_open() */
	DRM(vm_open)(vma);
	return 0;
}
@


1.1.9.1
log
@OpenBSD just has imported exactly this tree into their vendor branch,
called the same tag, in XF4/xc
This is, apparently, the last XFree86 snapshot before the licence change
(ie, addition of the advertising clause)

Since the developers don't see any problems with that, and we would like
to integrate improvements done by the remaining one or two (or so) XFree86
developers (j/k), this prepares enabling us to update X-Window in the future.
@
text
@d1 1
a1 9
/**
 * \file drm_vm.h
 * Memory mapping for DRM
 * 
 * \author Rickard E. (Rik) Faith <faith@@valinux.com>
 * \author Gareth Hughes <gareth@@valinux.com>
 */

/*
d26 4
d35 5
d41 21
a61 12
/**
 * \c nopage method for AGP virtual memory.
 *
 * \param vma virtual memory area.
 * \param address access address.
 * \return pointer to the page structure.
 * 
 * Find the right map and if it's AGP memory find the real physical page to
 * map, get the page, increment the use count and return it.
 */
static __inline__ struct page *DRM(do_vm_nopage)(struct vm_area_struct *vma,
						 unsigned long address)
d74 1
a74 1
	if(!dev->agp || !dev->agp->cant_use_aperture) goto vm_nopage_error;
d77 1
a77 1
		r_list = list_entry(list, drm_map_list_t, head);
d111 1
d115 2
a116 3
		DRM_DEBUG("baddr = 0x%lx page = 0x%p, offset = 0x%lx, count=%d\n",
			  baddr, __va(agpmem->memory->memory[offset]), offset,
			  atomic_read(&page->count));
d126 3
a128 12
/**
 * \c nopage method for shared virtual memory.
 *
 * \param vma virtual memory area.
 * \param address access address.
 * \return pointer to the page structure.
 * 
 * Get the the mapping, find the real physical page to map, get the page, and
 * return it.
 */
static __inline__ struct page *DRM(do_vm_shm_nopage)(struct vm_area_struct *vma,
						     unsigned long address)
d133 3
d143 11
a153 3
	page = vmalloc_to_page((void *)i);
	if (!page)
		return NOPAGE_OOM;
d156 1
a156 1
	DRM_DEBUG("shm_nopage 0x%lx\n", address);
d160 3
a163 8
/**
 * \c close method for shared virtual memory.
 * 
 * \param vma virtual memory area.
 * 
 * Deletes map information if we are the last
 * person to close a mapping and it's not in the global maplist.
 */
d204 1
a204 1
			r_list = list_entry(list, drm_map_list_t, head);
d221 1
a221 1
				DRM(ioremapfree)(map->handle, map->size, dev);
d236 3
a238 11
/**
 * \c nopage method for DMA virtual memory.
 *
 * \param vma virtual memory area.
 * \param address access address.
 * \return pointer to the page structure.
 * 
 * Determine the page number from the page offset and get it from drm_device_dma::pagelist.
 */
static __inline__ struct page *DRM(do_vm_dma_nopage)(struct vm_area_struct *vma,
						     unsigned long address)
d258 2
a259 1
	DRM_DEBUG("dma_nopage 0x%lx (page %lu)\n", address, page_nr);
d263 3
a265 11
/**
 * \c nopage method for scatter-gather virtual memory.
 *
 * \param vma virtual memory area.
 * \param address access address.
 * \return pointer to the page structure.
 * 
 * Determine the map offset from the page offset and get it from drm_sg_mem::pagelist.
 */
static __inline__ struct page *DRM(do_vm_sg_nopage)(struct vm_area_struct *vma,
						    unsigned long address)
a289 97

#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,0)

static struct page *DRM(vm_nopage)(struct vm_area_struct *vma,
				   unsigned long address,
				   int *type) {
	if (type) *type = VM_FAULT_MINOR;
	return DRM(do_vm_nopage)(vma, address);
}

static struct page *DRM(vm_shm_nopage)(struct vm_area_struct *vma,
				       unsigned long address,
				       int *type) {
	if (type) *type = VM_FAULT_MINOR;
	return DRM(do_vm_shm_nopage)(vma, address);
}

static struct page *DRM(vm_dma_nopage)(struct vm_area_struct *vma,
				       unsigned long address,
				       int *type) {
	if (type) *type = VM_FAULT_MINOR;
	return DRM(do_vm_dma_nopage)(vma, address);
}

static struct page *DRM(vm_sg_nopage)(struct vm_area_struct *vma,
				      unsigned long address,
				      int *type) {
	if (type) *type = VM_FAULT_MINOR;
	return DRM(do_vm_sg_nopage)(vma, address);
}

#else	/* LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,0) */

static struct page *DRM(vm_nopage)(struct vm_area_struct *vma,
				   unsigned long address,
				   int unused) {
	return DRM(do_vm_nopage)(vma, address);
}

static struct page *DRM(vm_shm_nopage)(struct vm_area_struct *vma,
				       unsigned long address,
				       int unused) {
	return DRM(do_vm_shm_nopage)(vma, address);
}

static struct page *DRM(vm_dma_nopage)(struct vm_area_struct *vma,
				       unsigned long address,
				       int unused) {
	return DRM(do_vm_dma_nopage)(vma, address);
}

static struct page *DRM(vm_sg_nopage)(struct vm_area_struct *vma,
				      unsigned long address,
				      int unused) {
	return DRM(do_vm_sg_nopage)(vma, address);
}

#endif


/** AGP virtual memory operations */
static struct vm_operations_struct   DRM(vm_ops) = {
	.nopage = DRM(vm_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_close),
};

/** Shared virtual memory operations */
static struct vm_operations_struct   DRM(vm_shm_ops) = {
	.nopage = DRM(vm_shm_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_shm_close),
};

/** DMA virtual memory operations */
static struct vm_operations_struct   DRM(vm_dma_ops) = {
	.nopage = DRM(vm_dma_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_close),
};

/** Scatter-gather virtual memory operations */
static struct vm_operations_struct   DRM(vm_sg_ops) = {
	.nopage = DRM(vm_sg_nopage),
	.open   = DRM(vm_open),
	.close  = DRM(vm_close),
};


/**
 * \c open method for shared virtual memory.
 * 
 * \param vma virtual memory area.
 * 
 * Create a new drm_vma_entry structure as the \p vma private data entry and
 * add it to drm_device::vmalist.
 */
a310 8
/**
 * \c close method for all virtual memory types.
 * 
 * \param vma virtual memory area.
 * 
 * Search the \p vma private data entry in drm_device::vmalist, unlink it, and
 * free it.
 */
a335 10
/**
 * mmap DMA memory.
 *
 * \param filp file pointer.
 * \param vma virtual memory area.
 * \return zero on success or a negative number on failure.
 * 
 * Sets the virtual memory area operations structure to vm_dma_ops, the file
 * pointer, and calls vm_open().
 */
d358 1
a358 1
#if LINUX_VERSION_CODE <= 0x02040e /* KERNEL_VERSION(2,4,14) */
a381 13
/**
 * mmap DMA memory.
 *
 * \param filp file pointer.
 * \param vma virtual memory area.
 * \return zero on success or a negative number on failure.
 * 
 * If the virtual memory area has no offset associated with it then it's a DMA
 * area, so calls mmap_dma(). Otherwise searches the map in drm_device::maplist,
 * checks that the restricted flag is not set, sets the virtual memory operations
 * according to the mapping type and remaps the pages. Finally sets the file
 * pointer and calls vm_open().
 */
d396 1
a396 10
	/* We check for "dma". On Apple's UniNorth, it's valid to have
	 * the AGP mapped at physical address 0
	 * --BenH.
	 */
	if (!VM_OFFSET(vma)
#if __REALLY_HAVE_AGP
	    && (!dev->agp || dev->agp->agp_info.device->vendor != PCI_VENDOR_ID_APPLE)
#endif
	    )
		return DRM(mmap_dma)(filp, vma);
d408 1
a408 1
		r_list = list_entry(list, drm_map_list_t, head);
d422 2
a423 2
		vma->vm_flags &= ~(VM_WRITE | VM_MAYWRITE);
#if defined(__i386__) || defined(__AMD64__)
d436 1
a436 2
#if __REALLY_HAVE_AGP
	  if (dev->agp->cant_use_aperture) {
d438 4
a441 3
                 * On some platforms we can't talk to bus dma address from the CPU, so for
                 * memory of type DRM_AGP, we'll deal with sorting out the real physical
                 * pages and mappings in nopage()
a442 3
#if defined(__powerpc__)
		pgprot_val(vma->vm_page_prot) |= _PAGE_NO_CACHE;
#endif
a444 1
	  }
d450 1
a450 1
#if defined(__i386__) || defined(__AMD64__)
d455 4
a463 4
#if defined(__ia64__)
		if (map->type != _DRM_AGP)
			vma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);
#endif
d465 1
a465 7
#ifdef __sparc__
		if (io_remap_page_range(DRM_RPR_ARG(vma) vma->vm_start,
					VM_OFFSET(vma) + offset,
					vma->vm_end - vma->vm_start,
					vma->vm_page_prot, 0))
#else
		if (remap_page_range(DRM_RPR_ARG(vma) vma->vm_start,
a468 1
#endif
d481 1
a481 1
#if LINUX_VERSION_CODE <= 0x02040e /* KERNEL_VERSION(2,4,14) */
d490 1
a490 1
#if LINUX_VERSION_CODE <= 0x02040e /* KERNEL_VERSION(2,4,14) */
d499 1
a499 1
#if LINUX_VERSION_CODE <= 0x02040e /* KERNEL_VERSION(2,4,14) */
@


1.1.1.1
log
@Import OpenBSD 3.3 XF4 repository from CTM 3132 the first time
This finalizes starting an OpenBSD-mirabile (aka MirBSD) repository.

### MirBSD is:
# Copyright (c) 1982-2003 by Thorsten "mirabile" Glaser <x86@@ePost.de>
# Copyright © 1968-2003  The authors of And contributors to UNIX®, the
#       C Language, BSD/Berkeley Unix; 386BSD, NetBSD 1.1 and OpenBSD.
#
# Anyone who obtained a copy of this work is hereby permitted to freely use,
# distribute, modify, merge, sublicence, give away or sell it as long as the
# authors are given due credit and the following notice is retained:
#
# This work is provided "as is", with no explicit or implicit warranty what-
# soever. Use it only at your own risk. In no event may an author or contri-
# butor be held liable for any damage, directly or indirectly, that origina-
# ted through or is caused by creation or modification of this work.

MirBSD is my private tree. MirBSD does not differ very much from OpenBSD
and intentionally tracks OpenBSD. That's why it _is_ OpenBSD, just not the
official one. It's like with DarrenBSD.

At time of this writing, no advertising for MirBSD must be done,
because the advertising clause has not yet been sorted out.

http://templeofhate.com/tglaser/MirBSD/index.php
@
text
@@


1.1.1.2
log
@The X-Windowing System

Import XFree86 4.3 from OpenBSD by CTM, in the hope it's stable
@
text
@d36 3
a38 3
	.nopage = DRM(vm_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_close),
d42 3
a44 3
	.nopage = DRM(vm_shm_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_shm_close),
d48 3
a50 3
	.nopage = DRM(vm_dma_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_close),
d54 3
a56 3
	.nopage = DRM(vm_sg_nopage),
	.open   = DRM(vm_open),
	.close  = DRM(vm_close),
d74 1
a74 1
	if(!dev->agp || !dev->agp->cant_use_aperture) goto vm_nopage_error;
d133 3
d143 11
a153 3
	page = vmalloc_to_page((void *)i);
	if (!page)
		return NOPAGE_OOM;
d156 1
a156 1
	DRM_DEBUG("shm_nopage 0x%lx\n", address);
d258 2
a259 1
	DRM_DEBUG("dma_nopage 0x%lx (page %lu)\n", address, page_nr);
d358 1
a358 1
#if LINUX_VERSION_CODE <= 0x02040e /* KERNEL_VERSION(2,4,14) */
d423 1
a423 1
#if defined(__i386__) || defined(__x86_64__)
d450 1
a450 1
#if defined(__i386__) || defined(__x86_64__)
d465 1
a465 7
#ifdef __sparc__
		if (io_remap_page_range(DRM_RPR_ARG(vma) vma->vm_start,
					VM_OFFSET(vma) + offset,
					vma->vm_end - vma->vm_start,
					vma->vm_page_prot, 0))
#else
		if (remap_page_range(DRM_RPR_ARG(vma) vma->vm_start,
a468 1
#endif
d481 1
a481 1
#if LINUX_VERSION_CODE <= 0x02040e /* KERNEL_VERSION(2,4,14) */
d490 1
a490 1
#if LINUX_VERSION_CODE <= 0x02040e /* KERNEL_VERSION(2,4,14) */
d499 1
a499 1
#if LINUX_VERSION_CODE <= 0x02040e /* KERNEL_VERSION(2,4,14) */
@


1.1.1.3
log
@That's what OpenBSD will, probably, ship as XF4 in 3.5
their last sync against XFree86 4.3-current has been
imported into our vendor branch, too
@
text
@d1 1
a1 9
/**
 * \file drm_vm.h
 * Memory mapping for DRM
 * 
 * \author Rickard E. (Rik) Faith <faith@@valinux.com>
 * \author Gareth Hughes <gareth@@valinux.com>
 */

/*
d26 4
d35 23
d59 3
a61 12
/**
 * \c nopage method for AGP virtual memory.
 *
 * \param vma virtual memory area.
 * \param address access address.
 * \return pointer to the page structure.
 * 
 * Find the right map and if it's AGP memory find the real physical page to
 * map, get the page, increment the use count and return it.
 */
static __inline__ struct page *DRM(do_vm_nopage)(struct vm_area_struct *vma,
						 unsigned long address)
d77 1
a77 1
		r_list = list_entry(list, drm_map_list_t, head);
d111 1
d115 2
a116 3
		DRM_DEBUG("baddr = 0x%lx page = 0x%p, offset = 0x%lx, count=%d\n",
			  baddr, __va(agpmem->memory->memory[offset]), offset,
			  atomic_read(&page->count));
d126 3
a128 12
/**
 * \c nopage method for shared virtual memory.
 *
 * \param vma virtual memory area.
 * \param address access address.
 * \return pointer to the page structure.
 * 
 * Get the the mapping, find the real physical page to map, get the page, and
 * return it.
 */
static __inline__ struct page *DRM(do_vm_shm_nopage)(struct vm_area_struct *vma,
						     unsigned long address)
d149 3
a152 8
/**
 * \c close method for shared virtual memory.
 * 
 * \param vma virtual memory area.
 * 
 * Deletes map information if we are the last
 * person to close a mapping and it's not in the global maplist.
 */
d193 1
a193 1
			r_list = list_entry(list, drm_map_list_t, head);
d210 1
a210 1
				DRM(ioremapfree)(map->handle, map->size, dev);
d225 3
a227 11
/**
 * \c nopage method for DMA virtual memory.
 *
 * \param vma virtual memory area.
 * \param address access address.
 * \return pointer to the page structure.
 * 
 * Determine the page number from the page offset and get it from drm_device_dma::pagelist.
 */
static __inline__ struct page *DRM(do_vm_dma_nopage)(struct vm_area_struct *vma,
						     unsigned long address)
d251 3
a253 11
/**
 * \c nopage method for scatter-gather virtual memory.
 *
 * \param vma virtual memory area.
 * \param address access address.
 * \return pointer to the page structure.
 * 
 * Determine the map offset from the page offset and get it from drm_sg_mem::pagelist.
 */
static __inline__ struct page *DRM(do_vm_sg_nopage)(struct vm_area_struct *vma,
						    unsigned long address)
a277 97

#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,0)

static struct page *DRM(vm_nopage)(struct vm_area_struct *vma,
				   unsigned long address,
				   int *type) {
	if (type) *type = VM_FAULT_MINOR;
	return DRM(do_vm_nopage)(vma, address);
}

static struct page *DRM(vm_shm_nopage)(struct vm_area_struct *vma,
				       unsigned long address,
				       int *type) {
	if (type) *type = VM_FAULT_MINOR;
	return DRM(do_vm_shm_nopage)(vma, address);
}

static struct page *DRM(vm_dma_nopage)(struct vm_area_struct *vma,
				       unsigned long address,
				       int *type) {
	if (type) *type = VM_FAULT_MINOR;
	return DRM(do_vm_dma_nopage)(vma, address);
}

static struct page *DRM(vm_sg_nopage)(struct vm_area_struct *vma,
				      unsigned long address,
				      int *type) {
	if (type) *type = VM_FAULT_MINOR;
	return DRM(do_vm_sg_nopage)(vma, address);
}

#else	/* LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,0) */

static struct page *DRM(vm_nopage)(struct vm_area_struct *vma,
				   unsigned long address,
				   int unused) {
	return DRM(do_vm_nopage)(vma, address);
}

static struct page *DRM(vm_shm_nopage)(struct vm_area_struct *vma,
				       unsigned long address,
				       int unused) {
	return DRM(do_vm_shm_nopage)(vma, address);
}

static struct page *DRM(vm_dma_nopage)(struct vm_area_struct *vma,
				       unsigned long address,
				       int unused) {
	return DRM(do_vm_dma_nopage)(vma, address);
}

static struct page *DRM(vm_sg_nopage)(struct vm_area_struct *vma,
				      unsigned long address,
				      int unused) {
	return DRM(do_vm_sg_nopage)(vma, address);
}

#endif


/** AGP virtual memory operations */
static struct vm_operations_struct   DRM(vm_ops) = {
	.nopage = DRM(vm_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_close),
};

/** Shared virtual memory operations */
static struct vm_operations_struct   DRM(vm_shm_ops) = {
	.nopage = DRM(vm_shm_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_shm_close),
};

/** DMA virtual memory operations */
static struct vm_operations_struct   DRM(vm_dma_ops) = {
	.nopage = DRM(vm_dma_nopage),
	.open	= DRM(vm_open),
	.close	= DRM(vm_close),
};

/** Scatter-gather virtual memory operations */
static struct vm_operations_struct   DRM(vm_sg_ops) = {
	.nopage = DRM(vm_sg_nopage),
	.open   = DRM(vm_open),
	.close  = DRM(vm_close),
};


/**
 * \c open method for shared virtual memory.
 * 
 * \param vma virtual memory area.
 * 
 * Create a new drm_vma_entry structure as the \p vma private data entry and
 * add it to drm_device::vmalist.
 */
a298 8
/**
 * \c close method for all virtual memory types.
 * 
 * \param vma virtual memory area.
 * 
 * Search the \p vma private data entry in drm_device::vmalist, unlink it, and
 * free it.
 */
a323 10
/**
 * mmap DMA memory.
 *
 * \param filp file pointer.
 * \param vma virtual memory area.
 * \return zero on success or a negative number on failure.
 * 
 * Sets the virtual memory area operations structure to vm_dma_ops, the file
 * pointer, and calls vm_open().
 */
a369 13
/**
 * mmap DMA memory.
 *
 * \param filp file pointer.
 * \param vma virtual memory area.
 * \return zero on success or a negative number on failure.
 * 
 * If the virtual memory area has no offset associated with it then it's a DMA
 * area, so calls mmap_dma(). Otherwise searches the map in drm_device::maplist,
 * checks that the restricted flag is not set, sets the virtual memory operations
 * according to the mapping type and remaps the pages. Finally sets the file
 * pointer and calls vm_open().
 */
d384 1
a384 10
	/* We check for "dma". On Apple's UniNorth, it's valid to have
	 * the AGP mapped at physical address 0
	 * --BenH.
	 */
	if (!VM_OFFSET(vma)
#if __REALLY_HAVE_AGP
	    && (!dev->agp || dev->agp->agp_info.device->vendor != PCI_VENDOR_ID_APPLE)
#endif
	    )
		return DRM(mmap_dma)(filp, vma);
d396 1
a396 1
		r_list = list_entry(list, drm_map_list_t, head);
d410 2
a411 2
		vma->vm_flags &= ~(VM_WRITE | VM_MAYWRITE);
#if defined(__i386__) || defined(__AMD64__)
d424 1
a424 2
#if __REALLY_HAVE_AGP
	  if (dev->agp->cant_use_aperture) {
d426 4
a429 3
                 * On some platforms we can't talk to bus dma address from the CPU, so for
                 * memory of type DRM_AGP, we'll deal with sorting out the real physical
                 * pages and mappings in nopage()
a430 3
#if defined(__powerpc__)
		pgprot_val(vma->vm_page_prot) |= _PAGE_NO_CACHE;
#endif
a432 1
	  }
d438 1
a438 1
#if defined(__i386__) || defined(__AMD64__)
d443 4
a451 4
#if defined(__ia64__)
		if (map->type != _DRM_AGP)
			vma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);
#endif
@


