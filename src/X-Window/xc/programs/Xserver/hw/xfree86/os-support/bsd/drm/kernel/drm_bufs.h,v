head	1.1;
branch	1.1.1;
access;
symbols
	tg-mergetmp-2:1.1.1.3
	cvs-200410241530:1.1.1.3
	cvs-200410012000:1.1.1.3
	cvs-200407221130:1.1.1.3
	cvs-200407141120:1.1.1.3
	cvs-200406231010:1.1.1.3
	cvs-200406052200:1.1.1.3
	MIRBSD_7quater:1.1.1.2
	cvs-200405271510:1.1.1.3
	XFree86_4_4_0:1.1.9.1
	cvs-200403021700:1.1.1.3
	XFREE86_20040213:1.1.9.1
	xc:1.1.9
	cvs-200401291925:1.1.1.2
	MIRBSD_7_ALPHA:1.1.1.2.0.4
	MIRBSD_7:1.1.1.2.0.2
	MIRBSD_7ter:1.1.1.2
	cvs-20011091815:1.1.1.2
	cvs-200309162130:1.1.1.2
	cvs-200308302005:1.1.1.2
	ctmx-0387:1.1.1.2
	ctmx-0384:1.1.1.2
	MIRBSD_5:1.1.1.2
	ctmx-0375:1.1.1.2
	ctmx-0373:1.1.1.2
	ctm-0371:1.1.1.2
	ctm-0370:1.1.1.2
	MIRBSD_4:1.1.1.2
	ctm-0363:1.1.1.2
	ctm-0359:1.1.1.2
	ctm-0349:1.1.1.1
	openbsd:1.1.1;
locks; strict;
comment	@ * @;


1.1
date	2003.03.22.20.08.32;	author tg;	state Exp;
branches
	1.1.1.1
	1.1.9.1;
next	;

1.1.1.1
date	2003.03.22.20.08.32;	author tg;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2003.04.08.18.37.29;	author tg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2004.03.02.18.24.34;	author tg;	state Stab;
branches;
next	;

1.1.9.1
date	2004.02.14.19.25.05;	author tg;	state Exp;
branches;
next	;


desc
@@


1.1
log
@Initial revision
@
text
@/* drm_bufs.h -- Generic buffer template -*- linux-c -*-
 * Created: Thu Nov 23 03:10:50 2000 by gareth@@valinux.com
 *
 * Copyright 1999, 2000 Precision Insight, Inc., Cedar Park, Texas.
 * Copyright 2000 VA Linux Systems, Inc., Sunnyvale, California.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 *
 * Authors:
 *    Rickard E. (Rik) Faith <faith@@valinux.com>
 *    Gareth Hughes <gareth@@valinux.com>
 */

#define __NO_VERSION__
#include <machine/param.h>
#include <sys/mman.h>
#include <vm/vm.h>
#include <vm/pmap.h>
#include <vm/vm_extern.h>
#include <vm/vm_map.h>
#include <vm/vm_param.h>
#include "drmP.h"

#ifndef __HAVE_PCI_DMA
#define __HAVE_PCI_DMA		0
#endif

#ifndef __HAVE_SG
#define __HAVE_SG		0
#endif

#ifndef DRIVER_BUF_PRIV_T
#define DRIVER_BUF_PRIV_T		u32
#endif
#ifndef DRIVER_AGP_BUFFERS_MAP
#if __HAVE_AGP && __HAVE_DMA
#error "You must define DRIVER_AGP_BUFFERS_MAP()"
#else
#define DRIVER_AGP_BUFFERS_MAP( dev )	NULL
#endif
#endif

/*
 * Compute order.  Can be made faster.
 */
int DRM(order)( unsigned long size )
{
	int order;
	unsigned long tmp;

	for ( order = 0, tmp = size ; tmp >>= 1 ; ++order );

	if ( size & ~(1 << order) )
		++order;

	return order;
}

int DRM(addmap)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_map_t *map;
	drm_map_list_entry_t *list;

	if (!(dev->flags & (FREAD|FWRITE)))
		DRM_OS_RETURN(EACCES); /* Require read/write */

	map = (drm_map_t *) DRM(alloc)( sizeof(*map), DRM_MEM_MAPS );
	if ( !map )
		DRM_OS_RETURN(ENOMEM);

	*map = *(drm_map_t *)data;

	/* Only allow shared memory to be removable since we only keep enough
	 * book keeping information about shared memory to allow for removal
	 * when processes fork.
	 */
	if ( (map->flags & _DRM_REMOVABLE) && map->type != _DRM_SHM ) {
		DRM(free)( map, sizeof(*map), DRM_MEM_MAPS );
		DRM_OS_RETURN(EINVAL);
	}
	DRM_DEBUG( "offset = 0x%08lx, size = 0x%08lx, type = %d\n",
		   map->offset, map->size, map->type );
	if ( (map->offset & PAGE_MASK) || (map->size & PAGE_MASK) ) {
		DRM(free)( map, sizeof(*map), DRM_MEM_MAPS );
		DRM_OS_RETURN(EINVAL);
	}
	map->mtrr   = -1;
	map->handle = 0;

	TAILQ_FOREACH(list, dev->maplist, link) {
		drm_map_t *entry = list->map;
		if (        (entry->offset >= map->offset
			    && (entry->offset) < (map->offset + map->size) )
			|| ((entry->offset + entry->size) >= map->offset
			    && (entry->offset + entry->size) < (map->offset + map->size) ) 
			|| ((entry->offset < map->offset)
			    && (entry->offset + entry->size) >= (map->offset + map->size) ) )
			DRM_DEBUG("map collission: add(0x%lx-0x%lx), current(0x%lx-0x%lx)\n", 
				entry->offset, entry->offset + entry->size - 1,
				map->offset, map->offset + map->size - 1);
	}

	switch ( map->type ) {
	case _DRM_REGISTERS:
	case _DRM_FRAME_BUFFER:
#if !defined(__sparc__) && !defined(__alpha__)
		if ( map->offset + map->size < map->offset
		) {
			DRM(free)( map, sizeof(*map), DRM_MEM_MAPS );
			DRM_OS_RETURN(EINVAL);
		}
#endif
#ifdef __alpha__
		map->offset += dev->hose->mem_space->start;
#endif
#if __REALLY_HAVE_MTRR
		if ( map->type == _DRM_FRAME_BUFFER ||
		     (map->flags & _DRM_WRITE_COMBINING) ) {
			map->mtrr = mtrr_add( map->offset, map->size,
					      MTRR_TYPE_WRCOMB, 1 );
		}
#endif
		map->handle = DRM(ioremap)( map->offset, map->size );
		break;

	case _DRM_SHM:
		DRM_INFO( "%ld %d %d\n",
			   map->size, DRM(order)( map->size ), PAGE_SHIFT);
		map->handle = (void *)DRM(alloc_pages)
			(DRM(order)(map->size) - PAGE_SHIFT, DRM_MEM_SAREA);
		DRM_DEBUG( "%ld %d %p\n",
			   map->size, DRM(order)( map->size ), map->handle );
		if ( !map->handle ) {
			DRM(free)( map, sizeof(*map), DRM_MEM_MAPS );
			DRM_OS_RETURN(ENOMEM);
		}
		map->offset = (unsigned long)map->handle;
		if ( map->flags & _DRM_CONTAINS_LOCK ) {
			dev->lock.hw_lock = map->handle; /* Pointer to lock */
		}
		break;
#if __REALLY_HAVE_AGP
	case _DRM_AGP:
#ifdef __alpha__
		map->offset += dev->hose->mem_space->start;
#endif
		map->offset += dev->agp->base;
		map->mtrr   = dev->agp->agp_mtrr; /* for getmap */
		break;
#endif
	case _DRM_SCATTER_GATHER:
		if (!dev->sg) {
			DRM(free)(map, sizeof(*map), DRM_MEM_MAPS);
			DRM_OS_RETURN(EINVAL);
		}
		map->offset = map->offset + dev->sg->handle;
		break;

	default:
		DRM(free)( map, sizeof(*map), DRM_MEM_MAPS );
		DRM_OS_RETURN(EINVAL);
	}

	list = DRM(alloc)(sizeof(*list), DRM_MEM_MAPS);
	if(!list) {
		DRM(free)(map, sizeof(*map), DRM_MEM_MAPS);
		DRM_OS_RETURN(EINVAL);
	}
	memset(list, 0, sizeof(*list));
	list->map = map;

	DRM_OS_LOCK;
	TAILQ_INSERT_TAIL(dev->maplist, list, link);
	DRM_OS_UNLOCK;

	*(drm_map_t *)data = *map;

	if ( map->type != _DRM_SHM ) {
		((drm_map_t *)data)->handle = (void *)map->offset;
	}
	return 0;
}


/* Remove a map private from list and deallocate resources if the mapping
 * isn't in use.
 */

int DRM(rmmap)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_map_list_entry_t *list;
	drm_map_t *map;
	drm_map_t request;
	int found_maps = 0;

	DRM_OS_KRNFROMUSR( request, (drm_map_t *)data, sizeof(request) );

	DRM_OS_LOCK;
	TAILQ_FOREACH(list, dev->maplist, link) {
		map = list->map;
		if(map->handle == request.handle &&
		   map->flags & _DRM_REMOVABLE) break;
	}

	/* List has wrapped around to the head pointer, or its empty we didn't
	 * find anything.
	 */
	if(list == NULL) {
		DRM_OS_UNLOCK;
		DRM_OS_RETURN(EINVAL);
	}
	TAILQ_REMOVE(dev->maplist, list, link);
	DRM(free)(list, sizeof(*list), DRM_MEM_MAPS);


	if(!found_maps) {
		switch (map->type) {
		case _DRM_REGISTERS:
		case _DRM_FRAME_BUFFER:
#if __REALLY_HAVE_MTRR
			if (map->mtrr >= 0) {
				int retcode;
				retcode = mtrr_del(map->mtrr,
						   map->offset,
						   map->size);
				DRM_DEBUG("mtrr_del = %d\n", retcode);
			}
#endif
			DRM(ioremapfree)(map->handle, map->size);
			break;
		case _DRM_SHM:
			DRM(free_pages)( (unsigned long)map->handle, DRM(order)(map->size), DRM_MEM_SAREA );
			break;
		case _DRM_AGP:
		case _DRM_SCATTER_GATHER:
			break;
		}
		DRM(free)(map, sizeof(*map), DRM_MEM_MAPS);
	}
	DRM_OS_UNLOCK;
	return 0;
}

#if __HAVE_DMA


static void DRM(cleanup_buf_error)(drm_buf_entry_t *entry)
{
	int i;

	if (entry->seg_count) {
		for (i = 0; i < entry->seg_count; i++) {
			DRM(free_pages)(entry->seglist[i],
					entry->page_order,
					DRM_MEM_DMA);
		}
		DRM(free)(entry->seglist,
			  entry->seg_count *
			  sizeof(*entry->seglist),
			  DRM_MEM_SEGS);

		entry->seg_count = 0;
	}

   	if(entry->buf_count) {
	   	for(i = 0; i < entry->buf_count; i++) {
			if(entry->buflist[i].dev_private) {
				DRM(free)(entry->buflist[i].dev_private,
					  entry->buflist[i].dev_priv_size,
					  DRM_MEM_BUFS);
			}
		}
		DRM(free)(entry->buflist,
			  entry->buf_count *
			  sizeof(*entry->buflist),
			  DRM_MEM_BUFS);

#if __HAVE_DMA_FREELIST
	   	DRM(freelist_destroy)(&entry->freelist);
#endif

		entry->buf_count = 0;
	}
}

#if __REALLY_HAVE_AGP
int DRM(addbufs_agp)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_device_dma_t *dma = dev->dma;
	drm_buf_desc_t request;
	drm_buf_entry_t *entry;
	drm_buf_t *buf;
	unsigned long offset;
	unsigned long agp_offset;
	int count;
	int order;
	int size;
	int alignment;
	int page_order;
	int total;
	int byte_count;
	int i;
	drm_buf_t **temp_buflist;

	if ( !dma ) DRM_OS_RETURN(EINVAL);

	DRM_OS_KRNFROMUSR( request, (drm_buf_desc_t *)data, sizeof(request) );

	count = request.count;
	order = DRM(order)( request.size );
	size = 1 << order;

	alignment  = (request.flags & _DRM_PAGE_ALIGN)
		? round_page(size) : size;
	page_order = order - PAGE_SHIFT > 0 ? order - PAGE_SHIFT : 0;
	total = PAGE_SIZE << page_order;

	byte_count = 0;
	agp_offset = dev->agp->base + request.agp_start;

	DRM_DEBUG( "count:      %d\n",  count );
	DRM_DEBUG( "order:      %d\n",  order );
	DRM_DEBUG( "size:       %d\n",  size );
	DRM_DEBUG( "agp_offset: 0x%lx\n", agp_offset );
	DRM_DEBUG( "alignment:  %d\n",  alignment );
	DRM_DEBUG( "page_order: %d\n",  page_order );
	DRM_DEBUG( "total:      %d\n",  total );

	if ( order < DRM_MIN_ORDER || order > DRM_MAX_ORDER ) 
		DRM_OS_RETURN(EINVAL);
	if ( dev->queue_count ) 
		DRM_OS_RETURN(EBUSY); /* Not while in use */

	DRM_OS_SPINLOCK( &dev->count_lock );
	if ( dev->buf_use ) {
		DRM_OS_SPINUNLOCK( &dev->count_lock );
		DRM_OS_RETURN(EBUSY);
	}
	atomic_inc( &dev->buf_alloc );
	DRM_OS_SPINUNLOCK( &dev->count_lock );

	DRM_OS_LOCK;
	entry = &dma->bufs[order];
	if ( entry->buf_count ) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM); /* May only call once for each order */
	}

	if (count < 0 || count > 4096) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(EINVAL);
	}

	entry->buflist = DRM(alloc)( count * sizeof(*entry->buflist),
				    DRM_MEM_BUFS );
	if ( !entry->buflist ) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);
	}
	memset( entry->buflist, 0, count * sizeof(*entry->buflist) );

	entry->buf_size = size;
	entry->page_order = page_order;

	offset = 0;

	while ( entry->buf_count < count ) {
		buf          = &entry->buflist[entry->buf_count];
		buf->idx     = dma->buf_count + entry->buf_count;
		buf->total   = alignment;
		buf->order   = order;
		buf->used    = 0;

		buf->offset  = (dma->byte_count + offset);
		buf->bus_address = agp_offset + offset;
		buf->address = (void *)(agp_offset + offset);
		buf->next    = NULL;
		buf->waiting = 0;
		buf->pending = 0;
		buf->dma_wait = 0;
		buf->pid     = 0;

		buf->dev_priv_size = sizeof(DRIVER_BUF_PRIV_T);
		buf->dev_private = DRM(alloc)( sizeof(DRIVER_BUF_PRIV_T),
					       DRM_MEM_BUFS );
		if(!buf->dev_private) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			DRM(cleanup_buf_error)(entry);
		}
		memset( buf->dev_private, 0, buf->dev_priv_size );

#if __HAVE_DMA_HISTOGRAM
		buf->time_queued = 0;
		buf->time_dispatched = 0;
		buf->time_completed = 0;
		buf->time_freed = 0;
#endif

		offset += alignment;
		entry->buf_count++;
		byte_count += PAGE_SIZE << page_order;
	}

	DRM_DEBUG( "byte_count: %d\n", byte_count );

	temp_buflist = DRM(realloc)( dma->buflist,
				     dma->buf_count * sizeof(*dma->buflist),
				     (dma->buf_count + entry->buf_count)
				     * sizeof(*dma->buflist),
				     DRM_MEM_BUFS );
	if(!temp_buflist) {
		/* Free the entry because it isn't valid */
		DRM(cleanup_buf_error)(entry);
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);
	}
	dma->buflist = temp_buflist;

	for ( i = 0 ; i < entry->buf_count ; i++ ) {
		dma->buflist[i + dma->buf_count] = &entry->buflist[i];
	}

	dma->buf_count += entry->buf_count;
	dma->byte_count += byte_count;

	DRM_DEBUG( "dma->buf_count : %d\n", dma->buf_count );
	DRM_DEBUG( "entry->buf_count : %d\n", entry->buf_count );

#if __HAVE_DMA_FREELIST
	DRM(freelist_create)( &entry->freelist, entry->buf_count );
	for ( i = 0 ; i < entry->buf_count ; i++ ) {
		DRM(freelist_put)( dev, &entry->freelist, &entry->buflist[i] );
	}
#endif
	DRM_OS_UNLOCK;

	request.count = entry->buf_count;
	request.size = size;

	DRM_OS_KRNTOUSR( (drm_buf_desc_t *)data, request, sizeof(request) );

	dma->flags = _DRM_DMA_USE_AGP;

	atomic_dec( &dev->buf_alloc );
	return 0;
}
#endif /* __REALLY_HAVE_AGP */

#if __HAVE_PCI_DMA
int DRM(addbufs_pci)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_device_dma_t *dma = dev->dma;
	drm_buf_desc_t request;
	int count;
	int order;
	int size;
	int total;
	int page_order;
	drm_buf_entry_t *entry;
	unsigned long page;
	drm_buf_t *buf;
	int alignment;
	unsigned long offset;
	int i;
	int byte_count;
	int page_count;
	unsigned long *temp_pagelist;
	drm_buf_t **temp_buflist;

	if ( !dma ) DRM_OS_RETURN(EINVAL);

	DRM_OS_KRNFROMUSR( request, (drm_buf_desc_t *)data, sizeof(request) );

	count = request.count;
	order = DRM(order)( request.size );
	size = 1 << order;

	DRM_DEBUG( "count=%d, size=%d (%d), order=%d, queue_count=%d\n",
		   request.count, request.size, size,
		   order, dev->queue_count );

	if ( order < DRM_MIN_ORDER || order > DRM_MAX_ORDER ) 
		DRM_OS_RETURN(EINVAL);
	if ( dev->queue_count ) 
		DRM_OS_RETURN(EBUSY); /* Not while in use */

	alignment = (request.flags & _DRM_PAGE_ALIGN)
		? round_page(size) : size;
	page_order = order - PAGE_SHIFT > 0 ? order - PAGE_SHIFT : 0;
	total = PAGE_SIZE << page_order;

	DRM_OS_SPINLOCK( &dev->count_lock );
	if ( dev->buf_use ) {
		DRM_OS_SPINUNLOCK( &dev->count_lock );
		DRM_OS_RETURN(EBUSY);
	}
	atomic_inc( &dev->buf_alloc );
	DRM_OS_SPINUNLOCK( &dev->count_lock );

	DRM_OS_LOCK;
	entry = &dma->bufs[order];
	if ( entry->buf_count ) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);	/* May only call once for each order */
	}

	if (count < 0 || count > 4096) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(EINVAL);
	}

	entry->buflist = DRM(alloc)( count * sizeof(*entry->buflist),
				    DRM_MEM_BUFS );
	if ( !entry->buflist ) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);
	}
	memset( entry->buflist, 0, count * sizeof(*entry->buflist) );

	entry->seglist = DRM(alloc)( count * sizeof(*entry->seglist),
				    DRM_MEM_SEGS );
	if ( !entry->seglist ) {
		DRM(free)( entry->buflist,
			  count * sizeof(*entry->buflist),
			  DRM_MEM_BUFS );
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);
	}
	memset( entry->seglist, 0, count * sizeof(*entry->seglist) );

	temp_pagelist = DRM(realloc)( dma->pagelist,
				      dma->page_count * sizeof(*dma->pagelist),
				      (dma->page_count + (count << page_order))
				      * sizeof(*dma->pagelist),
				      DRM_MEM_PAGES );
	if(!temp_pagelist) {
		DRM(free)( entry->buflist,
			   count * sizeof(*entry->buflist),
			   DRM_MEM_BUFS );
		DRM(free)( entry->seglist,
			   count * sizeof(*entry->seglist),
			   DRM_MEM_SEGS );
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);
	}

	dma->pagelist = temp_pagelist;
	DRM_DEBUG( "pagelist: %d entries\n",
		   dma->page_count + (count << page_order) );

	entry->buf_size	= size;
	entry->page_order = page_order;
	byte_count = 0;
	page_count = 0;

	while ( entry->buf_count < count ) {
		page = DRM(alloc_pages)( page_order, DRM_MEM_DMA );
		if ( !page ) break;
		entry->seglist[entry->seg_count++] = page;
		for ( i = 0 ; i < (1 << page_order) ; i++ ) {
			DRM_DEBUG( "page %d @@ 0x%08lx\n",
				   dma->page_count + page_count,
				   page + PAGE_SIZE * i );
			dma->pagelist[dma->page_count + page_count++]
				= page + PAGE_SIZE * i;
		}
		for ( offset = 0 ;
		      offset + size <= total && entry->buf_count < count ;
		      offset += alignment, ++entry->buf_count ) {
			buf	     = &entry->buflist[entry->buf_count];
			buf->idx     = dma->buf_count + entry->buf_count;
			buf->total   = alignment;
			buf->order   = order;
			buf->used    = 0;
			buf->offset  = (dma->byte_count + byte_count + offset);
			buf->address = (void *)(page + offset);
			buf->next    = NULL;
			buf->waiting = 0;
			buf->pending = 0;
			buf->dma_wait = 0;
			buf->pid     = 0;
#if __HAVE_DMA_HISTOGRAM
			buf->time_queued     = 0;
			buf->time_dispatched = 0;
			buf->time_completed  = 0;
			buf->time_freed      = 0;
#endif
			DRM_DEBUG( "buffer %d @@ %p\n",
				   entry->buf_count, buf->address );
		}
		byte_count += PAGE_SIZE << page_order;
	}

	temp_buflist = DRM(realloc)( dma->buflist,
				     dma->buf_count * sizeof(*dma->buflist),
				     (dma->buf_count + entry->buf_count)
				     * sizeof(*dma->buflist),
				     DRM_MEM_BUFS );
	if(!temp_buflist) {
		/* Free the entry because it isn't valid */
		DRM(cleanup_buf_error)(entry);
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);
	}
	dma->buflist = temp_buflist;

	for ( i = 0 ; i < entry->buf_count ; i++ ) {
		dma->buflist[i + dma->buf_count] = &entry->buflist[i];
	}

	dma->buf_count += entry->buf_count;
	dma->seg_count += entry->seg_count;
	dma->page_count += entry->seg_count << page_order;
	dma->byte_count += PAGE_SIZE * (entry->seg_count << page_order);

#if __HAVE_DMA_FREELIST
	DRM(freelist_create)( &entry->freelist, entry->buf_count );
	for ( i = 0 ; i < entry->buf_count ; i++ ) {
		DRM(freelist_put)( dev, &entry->freelist, &entry->buflist[i] );
	}
#endif
	DRM_OS_UNLOCK;

	request.count = entry->buf_count;
	request.size = size;

	DRM_OS_KRNTOUSR( (drm_buf_desc_t *)data, request, sizeof(request) );

	atomic_dec( &dev->buf_alloc );
	return 0;

}
#endif /* __HAVE_PCI_DMA */

#if __REALLY_HAVE_SG
int DRM(addbufs_sg)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_device_dma_t *dma = dev->dma;
	drm_buf_desc_t request;
	drm_buf_entry_t *entry;
	drm_buf_t *buf;
	unsigned long offset;
	unsigned long agp_offset;
	int count;
	int order;
	int size;
	int alignment;
	int page_order;
	int total;
	int byte_count;
	int i;
	drm_buf_t **temp_buflist;

	if ( !dma ) DRM_OS_RETURN(EINVAL);

	DRM_OS_KRNFROMUSR( request, (drm_buf_desc_t *)data, sizeof(request) );

	count = request.count;
	order = DRM(order)( request.size );
	size = 1 << order;

	alignment  = (request.flags & _DRM_PAGE_ALIGN)
		? round_page(size) : size;
	page_order = order - PAGE_SHIFT > 0 ? order - PAGE_SHIFT : 0;
	total = PAGE_SIZE << page_order;

	byte_count = 0;
	agp_offset = request.agp_start;

	DRM_DEBUG( "count:      %d\n",  count );
	DRM_DEBUG( "order:      %d\n",  order );
	DRM_DEBUG( "size:       %d\n",  size );
	DRM_DEBUG( "agp_offset: %ld\n", agp_offset );
	DRM_DEBUG( "alignment:  %d\n",  alignment );
	DRM_DEBUG( "page_order: %d\n",  page_order );
	DRM_DEBUG( "total:      %d\n",  total );

	if ( order < DRM_MIN_ORDER || order > DRM_MAX_ORDER ) 
		DRM_OS_RETURN(EINVAL);
	if ( dev->queue_count ) DRM_OS_RETURN(EBUSY); /* Not while in use */

	DRM_OS_SPINLOCK( &dev->count_lock );
	if ( dev->buf_use ) {
		DRM_OS_SPINUNLOCK( &dev->count_lock );
		DRM_OS_RETURN(EBUSY);
	}
	atomic_inc( &dev->buf_alloc );
	DRM_OS_SPINUNLOCK( &dev->count_lock );

	DRM_OS_LOCK;
	entry = &dma->bufs[order];
	if ( entry->buf_count ) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM); /* May only call once for each order */
	}

	if (count < 0 || count > 4096) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(EINVAL);
	}

	entry->buflist = DRM(alloc)( count * sizeof(*entry->buflist),
				     DRM_MEM_BUFS );
	if ( !entry->buflist ) {
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);
	}
	memset( entry->buflist, 0, count * sizeof(*entry->buflist) );

	entry->buf_size = size;
	entry->page_order = page_order;

	offset = 0;

	while ( entry->buf_count < count ) {
		buf          = &entry->buflist[entry->buf_count];
		buf->idx     = dma->buf_count + entry->buf_count;
		buf->total   = alignment;
		buf->order   = order;
		buf->used    = 0;

		buf->offset  = (dma->byte_count + offset);
		buf->bus_address = agp_offset + offset;
		buf->address = (void *)(agp_offset + offset + dev->sg->handle);
		buf->next    = NULL;
		buf->waiting = 0;
		buf->pending = 0;
		buf->dma_wait = 0;
		buf->pid     = 0;

		buf->dev_priv_size = sizeof(DRIVER_BUF_PRIV_T);
		buf->dev_private = DRM(alloc)( sizeof(DRIVER_BUF_PRIV_T),
					       DRM_MEM_BUFS );
		if(!buf->dev_private) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			DRM(cleanup_buf_error)(entry);
			DRM_OS_UNLOCK;
			atomic_dec( &dev->buf_alloc );
			DRM_OS_RETURN(ENOMEM);
		}

		memset( buf->dev_private, 0, buf->dev_priv_size );

# if __HAVE_DMA_HISTOGRAM
		buf->time_queued = 0;
		buf->time_dispatched = 0;
		buf->time_completed = 0;
		buf->time_freed = 0;
# endif
		DRM_DEBUG( "buffer %d @@ %p\n",
			   entry->buf_count, buf->address );

		offset += alignment;
		entry->buf_count++;
		byte_count += PAGE_SIZE << page_order;
	}

	DRM_DEBUG( "byte_count: %d\n", byte_count );

	temp_buflist = DRM(realloc)( dma->buflist,
				     dma->buf_count * sizeof(*dma->buflist),
				     (dma->buf_count + entry->buf_count)
				     * sizeof(*dma->buflist),
				     DRM_MEM_BUFS );
	if(!temp_buflist) {
		/* Free the entry because it isn't valid */
		DRM(cleanup_buf_error)(entry);
		DRM_OS_UNLOCK;
		atomic_dec( &dev->buf_alloc );
		DRM_OS_RETURN(ENOMEM);
	}
	dma->buflist = temp_buflist;

	for ( i = 0 ; i < entry->buf_count ; i++ ) {
		dma->buflist[i + dma->buf_count] = &entry->buflist[i];
	}

	dma->buf_count += entry->buf_count;
	dma->byte_count += byte_count;

	DRM_DEBUG( "dma->buf_count : %d\n", dma->buf_count );
	DRM_DEBUG( "entry->buf_count : %d\n", entry->buf_count );

#if __HAVE_DMA_FREELIST
	DRM(freelist_create)( &entry->freelist, entry->buf_count );
	for ( i = 0 ; i < entry->buf_count ; i++ ) {
		DRM(freelist_put)( dev, &entry->freelist, &entry->buflist[i] );
	}
#endif
	DRM_OS_UNLOCK;

	request.count = entry->buf_count;
	request.size = size;

	DRM_OS_KRNTOUSR( (drm_buf_desc_t *)data, request, sizeof(request) );

	dma->flags = _DRM_DMA_USE_SG;

	atomic_dec( &dev->buf_alloc );
	return 0;
}
#endif /* __REALLY_HAVE_SG */

int DRM(addbufs)( DRM_OS_IOCTL )
{
	drm_buf_desc_t request;

	DRM_OS_KRNFROMUSR( request, (drm_buf_desc_t *)data, sizeof(request) );

#if __REALLY_HAVE_AGP
	if ( request.flags & _DRM_AGP_BUFFER )
		return DRM(addbufs_agp)( kdev, cmd, data, flags, p );
	else
#endif
#if __REALLY_HAVE_SG
	if ( request.flags & _DRM_SG_BUFFER )
		return DRM(addbufs_sg)( kdev, cmd, data, flags, p );
	else
#endif
#if __HAVE_PCI_DMA
		return DRM(addbufs_pci)( kdev, cmd, data, flags, p );
#else
		DRM_OS_RETURN(EINVAL);
#endif
}

int DRM(infobufs)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_device_dma_t *dma = dev->dma;
	drm_buf_info_t request;
	int i;
	int count;

	if ( !dma ) DRM_OS_RETURN(EINVAL);

	DRM_OS_SPINLOCK( &dev->count_lock );
	if ( atomic_read( &dev->buf_alloc ) ) {
		DRM_OS_SPINUNLOCK( &dev->count_lock );
		DRM_OS_RETURN(EBUSY);
	}
	++dev->buf_use;		/* Can't allocate more after this call */
	DRM_OS_SPINUNLOCK( &dev->count_lock );

	DRM_OS_KRNFROMUSR( request, (drm_buf_info_t *)data, sizeof(request) );

	for ( i = 0, count = 0 ; i < DRM_MAX_ORDER + 1 ; i++ ) {
		if ( dma->bufs[i].buf_count ) ++count;
	}

	DRM_DEBUG( "count = %d\n", count );

	if ( request.count >= count ) {
		for ( i = 0, count = 0 ; i < DRM_MAX_ORDER + 1 ; i++ ) {
			if ( dma->bufs[i].buf_count ) {
				drm_buf_desc_t *to = &request.list[count];
				drm_buf_entry_t *from = &dma->bufs[i];
				drm_freelist_t *list = &dma->bufs[i].freelist;
				if ( DRM_OS_COPYTOUSR( &to->count,
						   &from->buf_count,
						   sizeof(from->buf_count) ) ||
				     DRM_OS_COPYTOUSR( &to->size,
						   &from->buf_size,
						   sizeof(from->buf_size) ) ||
				     DRM_OS_COPYTOUSR( &to->low_mark,
						   &list->low_mark,
						   sizeof(list->low_mark) ) ||
				     DRM_OS_COPYTOUSR( &to->high_mark,
						   &list->high_mark,
						   sizeof(list->high_mark) ) )
					DRM_OS_RETURN(EFAULT);

				DRM_DEBUG( "%d %d %d %d %d\n",
					   i,
					   dma->bufs[i].buf_count,
					   dma->bufs[i].buf_size,
					   dma->bufs[i].freelist.low_mark,
					   dma->bufs[i].freelist.high_mark );
				++count;
			}
		}
	}
	request.count = count;

	DRM_OS_KRNTOUSR( (drm_buf_info_t *)data, request, sizeof(request) );

	return 0;
}

int DRM(markbufs)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_device_dma_t *dma = dev->dma;
	drm_buf_desc_t request;
	int order;
	drm_buf_entry_t *entry;

	if ( !dma ) DRM_OS_RETURN(EINVAL);

	DRM_OS_KRNFROMUSR( request, (drm_buf_desc_t *)data, sizeof(request) );

	DRM_DEBUG( "%d, %d, %d\n",
		   request.size, request.low_mark, request.high_mark );
	order = DRM(order)( request.size );
	if ( order < DRM_MIN_ORDER || order > DRM_MAX_ORDER ) 
		DRM_OS_RETURN(EINVAL);
	entry = &dma->bufs[order];

	if ( request.low_mark < 0 || request.low_mark > entry->buf_count )
		DRM_OS_RETURN(EINVAL);
	if ( request.high_mark < 0 || request.high_mark > entry->buf_count )
		DRM_OS_RETURN(EINVAL);

	entry->freelist.low_mark  = request.low_mark;
	entry->freelist.high_mark = request.high_mark;

	return 0;
}

int DRM(freebufs)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_device_dma_t *dma = dev->dma;
	drm_buf_free_t request;
	int i;
	int idx;
	drm_buf_t *buf;

	if ( !dma ) DRM_OS_RETURN(EINVAL);

	DRM_OS_KRNFROMUSR( request, (drm_buf_free_t *)data, sizeof(request) );

	DRM_DEBUG( "%d\n", request.count );
	for ( i = 0 ; i < request.count ; i++ ) {
		if ( DRM_OS_COPYFROMUSR( &idx,
				     &request.list[i],
				     sizeof(idx) ) )
			DRM_OS_RETURN(EFAULT);
		if ( idx < 0 || idx >= dma->buf_count ) {
			DRM_ERROR( "Index %d (of %d max)\n",
				   idx, dma->buf_count - 1 );
			DRM_OS_RETURN(EINVAL);
		}
		buf = dma->buflist[idx];
		if ( buf->pid != DRM_OS_CURRENTPID ) {
			DRM_ERROR( "Process %d freeing buffer owned by %d\n",
				   DRM_OS_CURRENTPID, buf->pid );
			DRM_OS_RETURN(EINVAL);
		}
		DRM(free_buffer)( dev, buf );
	}

	return 0;
}

int DRM(mapbufs)( DRM_OS_IOCTL )
{
	DRM_OS_DEVICE;
	drm_device_dma_t *dma = dev->dma;
	int retcode = 0;
	const int zero = 0;
	vm_offset_t virtual, address;
#if __FreeBSD_version >= 500000
	struct vmspace *vms = p->td_proc->p_vmspace;
#else
	struct vmspace *vms = p->p_vmspace;
#endif
	drm_buf_map_t request;
	int i;

	if ( !dma ) DRM_OS_RETURN(EINVAL);

	DRM_OS_SPINLOCK( &dev->count_lock );
	if ( atomic_read( &dev->buf_alloc ) ) {
		DRM_OS_SPINUNLOCK( &dev->count_lock );
		DRM_OS_RETURN(EBUSY);
	}
	dev->buf_use++;		/* Can't allocate more after this call */
	DRM_OS_SPINUNLOCK( &dev->count_lock );

	DRM_OS_KRNFROMUSR( request, (drm_buf_map_t *)data, sizeof(request) );

	if ( request.count >= dma->buf_count ) {
		if ( (__HAVE_AGP && (dma->flags & _DRM_DMA_USE_AGP)) ||
		     (__HAVE_SG && (dma->flags & _DRM_DMA_USE_SG)) ) {
			drm_map_t *map = DRIVER_AGP_BUFFERS_MAP( dev );

			if ( !map ) {
				retcode = EINVAL;
				goto done;
			}

			virtual = round_page((vm_offset_t)vms->vm_daddr + MAXDSIZ);
			retcode = vm_mmap(&vms->vm_map,
					  &virtual,
					  round_page(map->size),
					  PROT_READ|PROT_WRITE, VM_PROT_ALL,
					  MAP_SHARED,
					  SLIST_FIRST(&kdev->si_hlist),
					  (unsigned long)map->offset );
		} else {
			virtual = round_page((vm_offset_t)vms->vm_daddr + MAXDSIZ);
			retcode = vm_mmap(&vms->vm_map,
					  &virtual,
					  round_page(dma->byte_count),
					  PROT_READ|PROT_WRITE, VM_PROT_ALL,
					  MAP_SHARED,
					  SLIST_FIRST(&kdev->si_hlist),
					  0);
		}
		if (retcode)
			goto done;
		request.virtual = (void *)virtual;

		for ( i = 0 ; i < dma->buf_count ; i++ ) {
			if ( DRM_OS_COPYTOUSR( &request.list[i].idx,
					   &dma->buflist[i]->idx,
					   sizeof(request.list[0].idx) ) ) {
				retcode = EFAULT;
				goto done;
			}
			if ( DRM_OS_COPYTOUSR( &request.list[i].total,
					   &dma->buflist[i]->total,
					   sizeof(request.list[0].total) ) ) {
				retcode = EFAULT;
				goto done;
			}
			if ( DRM_OS_COPYTOUSR( &request.list[i].used,
					   &zero,
					   sizeof(zero) ) ) {
				retcode = EFAULT;
				goto done;
			}
			address = virtual + dma->buflist[i]->offset; /* *** */
			if ( DRM_OS_COPYTOUSR( &request.list[i].address,
					   &address,
					   sizeof(address) ) ) {
				retcode = EFAULT;
				goto done;
			}
		}
	}
 done:
	request.count = dma->buf_count;

	DRM_DEBUG( "%d buffers, retcode = %d\n", request.count, retcode );

	DRM_OS_KRNTOUSR( (drm_buf_map_t *)data, request, sizeof(request) );

	DRM_OS_RETURN(retcode);
}

#endif /* __HAVE_DMA */

@


1.1.9.1
log
@OpenBSD just has imported exactly this tree into their vendor branch,
called the same tag, in XF4/xc
This is, apparently, the last XFree86 snapshot before the licence change
(ie, addition of the advertising clause)

Since the developers don't see any problems with that, and we would like
to integrate improvements done by the remaining one or two (or so) XFree86
developers (j/k), this prepares enabling us to update X-Window in the future.
@
text
@a29 1
 *
d32 8
d77 1
a77 1
int DRM(addmap)( DRM_IOCTL_ARGS )
d79 2
a80 3
	DRM_DEVICE;
	drm_map_t request;
	drm_local_map_t *map;
d82 1
a82 1
	
d84 1
a84 1
		return DRM_ERR(EACCES); /* Require read/write */
d86 3
a88 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_map_t *)data, sizeof(drm_map_t) );
d90 1
a90 3
	map = (drm_local_map_t *) DRM(alloc)( sizeof(*map), DRM_MEM_MAPS );
	if ( !map )
		return DRM_ERR(ENOMEM);
a91 7
	map->offset = request.offset;
	map->size = request.size;
	map->type = request.type;
	map->flags = request.flags;
	map->mtrr   = -1;
	map->handle = 0;
	
d98 1
a98 1
		return DRM_ERR(EINVAL);
d104 16
a119 1
		return DRM_ERR(EINVAL);
d125 3
a127 1
		if ( map->offset + map->size < map->offset ) {
d129 1
a129 1
			return DRM_ERR(EINVAL);
d131 4
d138 2
a139 6
			int mtrr;
			     
			mtrr = DRM(mtrr_add)(map->offset, map->size,
			     DRM_MTRR_WC);
			if (mtrr == 0)
				map->mtrr = 1;
d141 2
a142 2
#endif /* __REALLY_HAVE_MTRR */
		DRM_IOREMAP(map, dev);
d146 5
a150 2
		map->handle = (void *)DRM(alloc)(map->size, DRM_MEM_SAREA);
		DRM_DEBUG( "%lu %d %p\n",
d154 1
a154 1
			return DRM_ERR(ENOMEM);
a157 9
			/* Prevent a 2nd X Server from creating a 2nd lock */
			DRM_LOCK();
			if (dev->lock.hw_lock != NULL) {
				DRM_UNLOCK();
				DRM(free)(map->handle, map->size,
				    DRM_MEM_SAREA);
				DRM(free)(map, sizeof(*map), DRM_MEM_MAPS);
				return DRM_ERR(EBUSY);
			}
a158 1
			DRM_UNLOCK();
d163 3
d173 1
a173 1
			return DRM_ERR(EINVAL);
d180 1
a180 1
		return DRM_ERR(EINVAL);
d183 2
a184 2
	list = DRM(calloc)(1, sizeof(*list), DRM_MEM_MAPS);
	if (list == NULL) {
d186 1
a186 1
		return DRM_ERR(EINVAL);
d188 1
d191 1
a191 1
	DRM_LOCK();
d193 1
a193 1
	DRM_UNLOCK();
d195 1
a195 6
	request.offset = map->offset;
	request.size = map->size;
	request.type = map->type;
	request.flags = map->flags;
	request.mtrr   = map->mtrr;
	request.handle = map->handle;
d197 2
a198 2
	if ( request.type != _DRM_SHM ) {
		request.handle = (void *)request.offset;
a199 3

	DRM_COPY_TO_USER_IOCTL( (drm_map_t *)data, request, sizeof(drm_map_t) );

d208 1
a208 1
int DRM(rmmap)( DRM_IOCTL_ARGS )
d210 1
a210 1
	DRM_DEVICE;
d212 1
a212 1
	drm_local_map_t *map;
d214 1
d216 1
a216 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_map_t *)data, sizeof(request) );
d218 1
a218 1
	DRM_LOCK();
d221 2
a222 3
		if (map->handle == request.handle &&
		    map->flags & _DRM_REMOVABLE)
			break;
d225 6
a230 4
	/* No match found. */
	if (list == NULL) {
		DRM_UNLOCK();
		return DRM_ERR(EINVAL);
d233 1
a233 1
	DRM_UNLOCK();
a234 1
	DRM(free)(list, sizeof(*list), DRM_MEM_MAPS);
d236 4
a239 3
	switch (map->type) {
	case _DRM_REGISTERS:
	case _DRM_FRAME_BUFFER:
d241 16
a256 6
		if (map->mtrr >= 0) {
			int __unused mtrr;
			
			mtrr = DRM(mtrr_del)(map->offset, map->size,
			    DRM_MTRR_WC);
			DRM_DEBUG("mtrr_del = %d\n", mtrr);
d258 1
a258 9
#endif
		DRM(ioremapfree)(map);
		break;
	case _DRM_SHM:
		DRM(free)(map->handle, map->size, DRM_MEM_SAREA);
		break;
	case _DRM_AGP:
	case _DRM_SCATTER_GATHER:
		break;
d260 1
a260 1
	DRM(free)(map, sizeof(*map), DRM_MEM_MAPS);
d267 1
a267 1
static void DRM(cleanup_buf_error)(drm_device_t *dev, drm_buf_entry_t *entry)
a270 1
#if __HAVE_PCI_DMA
d273 3
a275 4
			if (entry->seglist[i] != NULL)
				DRM(pci_free)(dev, entry->buf_size,
				    (void *)entry->seglist[i],
				    entry->seglist_bus[i]);
a280 2
		DRM(free)(entry->seglist_bus, entry->seg_count *
			  sizeof(*entry->seglist_bus), DRM_MEM_SEGS);
a283 1
#endif /* __HAVE_PCI_DMA */
d285 7
a291 4
   	if (entry->buf_count) {
	   	for (i = 0; i < entry->buf_count; i++) {
			DRM(free)(entry->buflist[i].dev_private,
			    entry->buflist[i].dev_priv_size, DRM_MEM_BUFS);
d298 4
d307 1
a307 1
static int DRM(addbufs_agp)(drm_device_t *dev, drm_buf_desc_t *request)
d309 1
d311 1
d326 6
a331 2
	count = request->count;
	order = DRM(order)(request->size);
d334 1
a334 1
	alignment  = (request->flags & _DRM_PAGE_ALIGN)
d340 1
a340 1
	agp_offset = dev->agp->base + request->agp_start;
d350 14
d365 11
d380 3
a382 1
		return DRM_ERR(ENOMEM);
d402 1
d404 2
a405 1
		buf->filp    = NULL;
d408 3
a410 3
		buf->dev_private = DRM(calloc)(1, buf->dev_priv_size,
		    DRM_MEM_BUFS);
		if (buf->dev_private == NULL) {
d413 1
a413 2
			DRM(cleanup_buf_error)(dev, entry);
			return DRM_ERR(ENOMEM);
d415 8
d436 1
a436 1
	if (temp_buflist == NULL) {
d438 4
a441 2
		DRM(cleanup_buf_error)(dev, entry);
		return DRM_ERR(ENOMEM);
d455 12
a466 2
	request->count = entry->buf_count;
	request->size = size;
d470 1
d476 1
a476 1
static int DRM(addbufs_pci)(drm_device_t *dev, drm_buf_desc_t *request)
d478 1
d480 1
d487 1
a487 1
	vm_offset_t vaddr;
a495 1
	dma_addr_t bus_addr;
d497 6
a502 2
	count = request->count;
	order = DRM(order)(request->size);
d505 8
a512 2
	DRM_DEBUG( "count=%d, size=%d (%d), order=%d\n",
		   request->count, request->size, size, order );
d514 1
a514 1
	alignment = (request->flags & _DRM_PAGE_ALIGN)
d519 9
d529 5
d535 5
a539 6
	entry->buflist = DRM(alloc)(count * sizeof(*entry->buflist),
	    DRM_MEM_BUFS);
	entry->seglist = DRM(alloc)(count * sizeof(*entry->seglist),
	    DRM_MEM_SEGS);
	entry->seglist_bus = DRM(alloc)(count * sizeof(*entry->seglist_bus),
	    DRM_MEM_SEGS);
d541 8
a548 5
	/* Keep the original pagelist until we know all the allocations
	 * have succeeded
	 */
	temp_pagelist = DRM(alloc)((dma->page_count + (count << page_order)) *
	    sizeof(*dma->pagelist), DRM_MEM_PAGES);
d550 28
a577 16
	if (entry->buflist == NULL || entry->seglist == NULL || 
	    temp_pagelist == NULL) {
		DRM(free)(entry->buflist, count * sizeof(*entry->buflist),
		    DRM_MEM_BUFS);
		DRM(free)(entry->seglist, count * sizeof(*entry->seglist),
		    DRM_MEM_SEGS);
		DRM(free)(entry->seglist_bus, count *
		    sizeof(*entry->seglist_bus), DRM_MEM_SEGS);
		return DRM_ERR(ENOMEM);
	}

	bzero(entry->buflist, count * sizeof(*entry->buflist));
	bzero(entry->seglist, count * sizeof(*entry->seglist));
	
	memcpy(temp_pagelist, dma->pagelist, dma->page_count * 
	    sizeof(*dma->pagelist));
d579 1
d589 3
a591 15
		vaddr = (vm_offset_t) DRM(pci_alloc)(dev, size, alignment,
		    0xfffffffful, &bus_addr);
		if (vaddr == NULL) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			entry->seg_count = count;
			DRM(cleanup_buf_error)(dev, entry);
			DRM(free)(temp_pagelist, (dma->page_count +
			    (count << page_order)) * sizeof(*dma->pagelist),
			    DRM_MEM_PAGES);
			return DRM_ERR(ENOMEM);
		}
	
		entry->seglist_bus[entry->seg_count] = bus_addr;
		entry->seglist[entry->seg_count++] = vaddr;
d595 3
a597 3
				   (long)vaddr + PAGE_SIZE * i );
			temp_pagelist[dma->page_count + page_count++] = 
			    vaddr + PAGE_SIZE * i;
d608 1
a608 2
			buf->address = (void *)(vaddr + offset);
			buf->bus_address = bus_addr + offset;
d610 1
d612 8
a619 17
			buf->filp    = NULL;

			buf->dev_priv_size = sizeof(DRIVER_BUF_PRIV_T);
			buf->dev_private = DRM(alloc)(sizeof(DRIVER_BUF_PRIV_T),
			    DRM_MEM_BUFS);
			if (buf->dev_private == NULL) {
				/* Set count correctly so we free the proper amount. */
				entry->buf_count = count;
				entry->seg_count = count;
				DRM(cleanup_buf_error)(dev, entry);
				DRM(free)(temp_pagelist, (dma->page_count + 
				    (count << page_order)) *
				    sizeof(*dma->pagelist), DRM_MEM_PAGES );
				return DRM_ERR(ENOMEM);
			}
			bzero(buf->dev_private, buf->dev_priv_size);

d631 1
a631 1
	if (temp_buflist == NULL) {
d633 4
a636 5
		DRM(cleanup_buf_error)(dev, entry);
		DRM(free)(temp_pagelist, (dma->page_count + 
		    (count << page_order)) * sizeof(*dma->pagelist),
		    DRM_MEM_PAGES);
		return DRM_ERR(ENOMEM);
a643 7
	/* No allocations failed, so now we can replace the orginal pagelist
	 * with the new one.
	 */
	DRM(free)(dma->pagelist, dma->page_count * sizeof(*dma->pagelist),
	    DRM_MEM_PAGES);
	dma->pagelist = temp_pagelist;

d649 12
a660 2
	request->count = entry->buf_count;
	request->size = size;
d662 1
d669 1
a669 1
static int DRM(addbufs_sg)(drm_device_t *dev, drm_buf_desc_t *request)
d671 1
d673 1
d688 6
a693 2
	count = request->count;
	order = DRM(order)(request->size);
d696 1
a696 1
	alignment  = (request->flags & _DRM_PAGE_ALIGN)
d702 1
a702 1
	agp_offset = request->agp_start;
d712 13
d726 11
d738 8
a745 4
	entry->buflist = DRM(calloc)(1, count * sizeof(*entry->buflist),
	    DRM_MEM_BUFS);
	if (entry->buflist == NULL)
		return DRM_ERR(ENOMEM);
d763 1
d765 2
a766 1
		buf->filp    = NULL;
d769 3
a771 3
		buf->dev_private = DRM(calloc)(1, buf->dev_priv_size,
		    DRM_MEM_BUFS);
		if (buf->dev_private == NULL) {
d774 4
a777 2
			DRM(cleanup_buf_error)(dev, entry);
			return DRM_ERR(ENOMEM);
d780 8
d803 1
a803 1
	if (temp_buflist == NULL) {
d805 4
a808 2
		DRM(cleanup_buf_error)(dev, entry);
		return DRM_ERR(ENOMEM);
d822 12
a833 2
	request->count = entry->buf_count;
	request->size = size;
d837 1
d842 1
a842 1
int DRM(addbufs)( DRM_IOCTL_ARGS )
a843 1
	DRM_DEVICE;
a844 2
	int err;
	int order;
d846 1
a846 20
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_desc_t *)data, sizeof(request) );

	if (request.count < 0 || request.count > 4096)
		return DRM_ERR(EINVAL);

	order = DRM(order)(request.size);
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER)
		return DRM_ERR(EINVAL);

	DRM_SPINLOCK(&dev->dma_lock);
	/* No more allocations after first buffer-using ioctl. */
	if (dev->buf_use != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return DRM_ERR(EBUSY);
	}
	/* No more than one allocation per order */
	if (dev->dma->bufs[order].buf_count != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return DRM_ERR(ENOMEM);
	}
d850 1
a850 1
		err = DRM(addbufs_agp)(dev, &request);
d855 1
a855 1
		err = DRM(addbufs_sg)(dev, &request);
d859 1
a859 1
		err = DRM(addbufs_pci)(dev, &request);
d861 1
a861 1
		err = DRM_ERR(EINVAL);
a862 5
	DRM_SPINUNLOCK(&dev->dma_lock);

	DRM_COPY_TO_USER_IOCTL((drm_buf_desc_t *)data, request, sizeof(request));

	return err;
d865 1
a865 1
int DRM(infobufs)( DRM_IOCTL_ARGS )
d867 1
a867 1
	DRM_DEVICE;
a871 1
	int retcode = 0;
d873 1
a873 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_info_t *)data, sizeof(request) );
d875 5
a879 1
	DRM_SPINLOCK(&dev->dma_lock);
d881 3
a883 1
	DRM_SPINUNLOCK(&dev->dma_lock);
d894 16
a909 12
				drm_buf_desc_t from;

				from.count = dma->bufs[i].buf_count;
				from.size = dma->bufs[i].buf_size;
				from.low_mark = dma->bufs[i].freelist.low_mark;
				from.high_mark = dma->bufs[i].freelist.high_mark;

				if (DRM_COPY_TO_USER(&request.list[count], &from,
				    sizeof(drm_buf_desc_t)) != 0) {
					retcode = DRM_ERR(EFAULT);
					break;
				}
d923 1
a923 1
	DRM_COPY_TO_USER_IOCTL( (drm_buf_info_t *)data, request, sizeof(request) );
d925 1
a925 1
	return retcode;
d928 1
a928 1
int DRM(markbufs)( DRM_IOCTL_ARGS )
d930 1
a930 1
	DRM_DEVICE;
d934 3
d938 1
a938 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_desc_t *)data, sizeof(request) );
d942 4
a945 7
	

	order = DRM(order)(request.size);	
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER ||
	    request.low_mark < 0 || request.high_mark < 0) {
		return DRM_ERR(EINVAL);
	}
d947 4
a950 5
	DRM_SPINLOCK(&dev->dma_lock);
	if (request.low_mark > dma->bufs[order].buf_count ||
	    request.high_mark > dma->bufs[order].buf_count) {
		return DRM_ERR(EINVAL);
	}
d952 2
a953 3
	dma->bufs[order].freelist.low_mark  = request.low_mark;
	dma->bufs[order].freelist.high_mark = request.high_mark;
	DRM_SPINUNLOCK(&dev->dma_lock);
d958 1
a958 1
int DRM(freebufs)( DRM_IOCTL_ARGS )
d960 1
a960 1
	DRM_DEVICE;
a965 1
	int retcode = 0;
d967 3
a969 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_free_t *)data, sizeof(request) );
a971 2
	
	DRM_SPINLOCK(&dev->dma_lock);
d973 4
a976 4
		if (DRM_COPY_FROM_USER(&idx, &request.list[i], sizeof(idx))) {
			retcode = DRM_ERR(EFAULT);
			break;
		}
d980 1
a980 2
			retcode = DRM_ERR(EINVAL);
			break;
d983 4
a986 5
		if ( buf->filp != filp ) {
			DRM_ERROR("Process %d freeing buffer not owned\n",
				   DRM_CURRENTPID);
			retcode = DRM_ERR(EINVAL);
			break;
a989 1
	DRM_SPINUNLOCK(&dev->dma_lock);
d991 1
a991 1
	return retcode;
d994 1
a994 1
int DRM(mapbufs)( DRM_IOCTL_ARGS )
d996 1
a996 1
	DRM_DEVICE;
d1000 6
a1005 13
	vm_offset_t address;
	struct vmspace *vms;
#ifdef __FreeBSD__
	vm_ooffset_t foff;
	vm_size_t size;
	vm_offset_t vaddr;
#endif /* __FreeBSD__ */
#ifdef __NetBSD__
	struct vnode *vn;
	vm_size_t size;
	vaddr_t vaddr;
#endif /* __NetBSD__ */

d1009 1
a1009 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_map_t *)data, sizeof(request) );
d1011 5
a1015 12
#ifdef __NetBSD__
	if (!vfinddev(kdev, VCHR, &vn))
		return 0;	/* FIXME: Shouldn't this be EINVAL or something? */
#endif /* __NetBSD__ */

#if defined(__FreeBSD__) && __FreeBSD_version >= 500000
	vms = p->td_proc->p_vmspace;
#else
	vms = p->p_vmspace;
#endif

	DRM_SPINLOCK(&dev->dma_lock);
d1017 1
a1017 1
	DRM_SPINUNLOCK(&dev->dma_lock);
d1019 1
a1019 2
	if (request.count < dma->buf_count)
		goto done;
d1021 9
a1029 3
	if ((__HAVE_AGP && (dma->flags & _DRM_DMA_USE_AGP)) ||
	    (__HAVE_SG && (dma->flags & _DRM_DMA_USE_SG))) {
		drm_local_map_t *map = DRIVER_AGP_BUFFERS_MAP(dev);
d1031 17
a1047 3
		if (map == NULL) {
			retcode = EINVAL;
			goto done;
d1049 1
a1049 42
		size = round_page(map->size);
		foff = map->offset;
	} else {
		size = round_page(dma->byte_count),
		foff = 0;
	}

#ifdef __FreeBSD__
	vaddr = round_page((vm_offset_t)vms->vm_daddr + MAXDSIZ);
	retcode = vm_mmap(&vms->vm_map, &vaddr, size, PROT_READ | PROT_WRITE,
	    VM_PROT_ALL, MAP_SHARED, SLIST_FIRST(&kdev->si_hlist), foff );
#elif defined(__NetBSD__)
	vaddr = round_page((vaddr_t)vms->vm_daddr + MAXDSIZ);
	retcode = uvm_mmap(&vms->vm_map, &vaddr, size,
	    UVM_PROT_READ | UVM_PROT_WRITE, UVM_PROT_ALL, MAP_SHARED,
	    &vn->v_uobj, foff, p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
#endif /* __NetBSD__ */
	if (retcode)
		goto done;

	request.virtual = (void *)vaddr;

	for ( i = 0 ; i < dma->buf_count ; i++ ) {
		if (DRM_COPY_TO_USER(&request.list[i].idx,
		    &dma->buflist[i]->idx, sizeof(request.list[0].idx))) {
			retcode = EFAULT;
			goto done;
		}
		if (DRM_COPY_TO_USER(&request.list[i].total,
		    &dma->buflist[i]->total, sizeof(request.list[0].total))) {
			retcode = EFAULT;
			goto done;
		}
		if (DRM_COPY_TO_USER(&request.list[i].used, &zero,
		    sizeof(zero))) {
			retcode = EFAULT;
			goto done;
		}
		address = vaddr + dma->buflist[i]->offset; /* *** */
		if (DRM_COPY_TO_USER(&request.list[i].address, &address,
		    sizeof(address))) {
			retcode = EFAULT;
d1051 28
a1080 1

d1086 1
a1086 1
	DRM_COPY_TO_USER_IOCTL((drm_buf_map_t *)data, request, sizeof(request));
d1088 1
a1088 1
	return DRM_ERR(retcode);
d1092 1
@


1.1.1.1
log
@Import OpenBSD 3.3 XF4 repository from CTM 3132 the first time
This finalizes starting an OpenBSD-mirabile (aka MirBSD) repository.

### MirBSD is:
# Copyright (c) 1982-2003 by Thorsten "mirabile" Glaser <x86@@ePost.de>
# Copyright © 1968-2003  The authors of And contributors to UNIX®, the
#       C Language, BSD/Berkeley Unix; 386BSD, NetBSD 1.1 and OpenBSD.
#
# Anyone who obtained a copy of this work is hereby permitted to freely use,
# distribute, modify, merge, sublicence, give away or sell it as long as the
# authors are given due credit and the following notice is retained:
#
# This work is provided "as is", with no explicit or implicit warranty what-
# soever. Use it only at your own risk. In no event may an author or contri-
# butor be held liable for any damage, directly or indirectly, that origina-
# ted through or is caused by creation or modification of this work.

MirBSD is my private tree. MirBSD does not differ very much from OpenBSD
and intentionally tracks OpenBSD. That's why it _is_ OpenBSD, just not the
official one. It's like with DarrenBSD.

At time of this writing, no advertising for MirBSD must be done,
because the advertising clause has not yet been sorted out.

http://templeofhate.com/tglaser/MirBSD/index.php
@
text
@@


1.1.1.2
log
@The X-Windowing System

Import XFree86 4.3 from OpenBSD by CTM, in the hope it's stable
@
text
@d32 8
d77 1
a77 1
int DRM(addmap)( DRM_IOCTL_ARGS )
d79 1
a79 1
	DRM_DEVICE;
d82 1
a82 1
	
d84 1
a84 1
		return DRM_ERR(EACCES); /* Require read/write */
d88 1
a88 1
		return DRM_ERR(ENOMEM);
d98 1
a98 1
		return DRM_ERR(EINVAL);
d104 1
a104 1
		return DRM_ERR(EINVAL);
d109 13
d129 1
a129 1
			return DRM_ERR(EINVAL);
d138 3
a140 20
#ifdef __FreeBSD__
			int retcode = 0, act;
			struct mem_range_desc mrdesc;
			mrdesc.mr_base = map->offset;
			mrdesc.mr_len = map->size;
			mrdesc.mr_flags = MDF_WRITECOMBINE;
			act = MEMRANGE_SET_UPDATE;
			bcopy(DRIVER_NAME, &mrdesc.mr_owner, strlen(DRIVER_NAME));
			retcode = mem_range_attr_set(&mrdesc, &act);
			map->mtrr=1;
#elif defined __NetBSD__
			struct mtrr mtrrmap;
			int one = 1;
			mtrrmap.base = map->offset;
			mtrrmap.len = map->size;
			mtrrmap.type = MTRR_TYPE_WC;
			mtrrmap.flags = MTRR_PRIVATE | MTRR_VALID;
			mtrrmap.owner = p->p_pid;
			/* USER? KERNEL? XXX */
			map->mtrr = mtrr_get( &mtrrmap, &one, p, MTRR_GETSET_KERNEL );
a141 2
		}
#endif /* __REALLY_HAVE_MTRR */
d146 4
a149 1
		map->handle = (void *)DRM(alloc)(map->size, DRM_MEM_SAREA);
d154 1
a154 1
			return DRM_ERR(ENOMEM);
d173 1
a173 1
			return DRM_ERR(EINVAL);
d180 1
a180 1
		return DRM_ERR(EINVAL);
d186 1
a186 1
		return DRM_ERR(EINVAL);
d191 1
a191 1
	DRM_LOCK;
d193 1
a193 1
	DRM_UNLOCK;
d208 1
a208 1
int DRM(rmmap)( DRM_IOCTL_ARGS )
d210 1
a210 1
	DRM_DEVICE;
d216 1
a216 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_map_t *)data, sizeof(request) );
d218 1
a218 1
	DRM_LOCK;
d229 2
a230 2
		DRM_UNLOCK;
		return DRM_ERR(EINVAL);
d243 3
a245 18
#ifdef __FreeBSD__
				int act;
				struct mem_range_desc mrdesc;
				mrdesc.mr_base = map->offset;
				mrdesc.mr_len = map->size;
				mrdesc.mr_flags = MDF_WRITECOMBINE;
				act = MEMRANGE_SET_REMOVE;
				bcopy(DRIVER_NAME, &mrdesc.mr_owner, strlen(DRIVER_NAME));
				retcode = mem_range_attr_set(&mrdesc, &act);
#elif defined __NetBSD__
				struct mtrr mtrrmap;
				int one = 1;
				mtrrmap.base = map->offset;
				mtrrmap.len = map->size;
				mtrrmap.type = 0;
				mtrrmap.flags = 0;
				mtrrmap.owner = p->p_pid;
				retcode = mtrr_set( &mtrrmap, &one, p, MTRR_GETSET_KERNEL);
a246 1
#endif
d252 1
a252 1
			DRM(free)( map->handle, map->size, DRM_MEM_SAREA );
d260 1
a260 1
	DRM_UNLOCK;
d273 2
a274 2
			DRM(free)((void *)entry->seglist[i],
					entry->buf_size,
d307 1
a307 1
int DRM(addbufs_agp)( DRM_IOCTL_ARGS )
d309 1
a309 1
	DRM_DEVICE;
d326 1
a326 1
	if ( !dma ) return DRM_ERR(EINVAL);
d328 1
a328 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_desc_t *)data, sizeof(request) );
d351 1
a351 1
		return DRM_ERR(EINVAL);
d353 1
a353 1
		return DRM_ERR(EBUSY); /* Not while in use */
d355 1
a355 1
	DRM_SPINLOCK( &dev->count_lock );
d357 2
a358 2
		DRM_SPINUNLOCK( &dev->count_lock );
		return DRM_ERR(EBUSY);
d361 1
a361 1
	DRM_SPINUNLOCK( &dev->count_lock );
d363 1
a363 1
	DRM_LOCK;
d366 1
a366 1
		DRM_UNLOCK;
d368 1
a368 1
		return DRM_ERR(ENOMEM); /* May only call once for each order */
d372 1
a372 1
		DRM_UNLOCK;
d374 1
a374 1
		return DRM_ERR(EINVAL);
d380 1
a380 1
		DRM_UNLOCK;
d382 1
a382 1
		return DRM_ERR(ENOMEM);
d439 1
a439 1
		DRM_UNLOCK;
d441 1
a441 1
		return DRM_ERR(ENOMEM);
d461 1
a461 1
	DRM_UNLOCK;
d466 1
a466 1
	DRM_COPY_TO_USER_IOCTL( (drm_buf_desc_t *)data, request, sizeof(request) );
d476 1
a476 1
int DRM(addbufs_pci)( DRM_IOCTL_ARGS )
d478 1
a478 1
	DRM_DEVICE;
d497 1
a497 1
	if ( !dma ) return DRM_ERR(EINVAL);
d499 1
a499 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_desc_t *)data, sizeof(request) );
d510 1
a510 1
		return DRM_ERR(EINVAL);
d512 1
a512 1
		return DRM_ERR(EBUSY); /* Not while in use */
d519 1
a519 1
	DRM_SPINLOCK( &dev->count_lock );
d521 2
a522 2
		DRM_SPINUNLOCK( &dev->count_lock );
		return DRM_ERR(EBUSY);
d525 1
a525 1
	DRM_SPINUNLOCK( &dev->count_lock );
d527 1
a527 1
	DRM_LOCK;
d530 1
a530 1
		DRM_UNLOCK;
d532 1
a532 1
		return DRM_ERR(ENOMEM);	/* May only call once for each order */
d536 1
a536 1
		DRM_UNLOCK;
d538 1
a538 1
		return DRM_ERR(EINVAL);
d544 1
a544 1
		DRM_UNLOCK;
d546 1
a546 1
		return DRM_ERR(ENOMEM);
d556 1
a556 1
		DRM_UNLOCK;
d558 1
a558 1
		return DRM_ERR(ENOMEM);
d574 1
a574 1
		DRM_UNLOCK;
d576 1
a576 1
		return DRM_ERR(ENOMEM);
d589 1
a589 1
		page = (unsigned long)DRM(alloc)( size, DRM_MEM_DMA );
d634 1
a634 1
		DRM_UNLOCK;
d636 1
a636 1
		return DRM_ERR(ENOMEM);
d655 1
a655 1
	DRM_UNLOCK;
d660 1
a660 1
	DRM_COPY_TO_USER_IOCTL( (drm_buf_desc_t *)data, request, sizeof(request) );
d669 1
a669 1
int DRM(addbufs_sg)( DRM_IOCTL_ARGS )
d671 1
a671 1
	DRM_DEVICE;
d688 1
a688 1
	if ( !dma ) return DRM_ERR(EINVAL);
d690 1
a690 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_desc_t *)data, sizeof(request) );
d713 2
a714 2
		return DRM_ERR(EINVAL);
	if ( dev->queue_count ) return DRM_ERR(EBUSY); /* Not while in use */
d716 1
a716 1
	DRM_SPINLOCK( &dev->count_lock );
d718 2
a719 2
		DRM_SPINUNLOCK( &dev->count_lock );
		return DRM_ERR(EBUSY);
d722 1
a722 1
	DRM_SPINUNLOCK( &dev->count_lock );
d724 1
a724 1
	DRM_LOCK;
d727 1
a727 1
		DRM_UNLOCK;
d729 1
a729 1
		return DRM_ERR(ENOMEM); /* May only call once for each order */
d733 1
a733 1
		DRM_UNLOCK;
d735 1
a735 1
		return DRM_ERR(EINVAL);
d741 1
a741 1
		DRM_UNLOCK;
d743 1
a743 1
		return DRM_ERR(ENOMEM);
d775 1
a775 1
			DRM_UNLOCK;
d777 1
a777 1
			return DRM_ERR(ENOMEM);
d806 1
a806 1
		DRM_UNLOCK;
d808 1
a808 1
		return DRM_ERR(ENOMEM);
d828 1
a828 1
	DRM_UNLOCK;
d833 1
a833 1
	DRM_COPY_TO_USER_IOCTL( (drm_buf_desc_t *)data, request, sizeof(request) );
d842 1
a842 1
int DRM(addbufs)( DRM_IOCTL_ARGS )
d846 1
a846 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_desc_t *)data, sizeof(request) );
d861 1
a861 1
		return DRM_ERR(EINVAL);
d865 1
a865 1
int DRM(infobufs)( DRM_IOCTL_ARGS )
d867 1
a867 1
	DRM_DEVICE;
d873 1
a873 1
	if ( !dma ) return DRM_ERR(EINVAL);
d875 1
a875 1
	DRM_SPINLOCK( &dev->count_lock );
d877 2
a878 2
		DRM_SPINUNLOCK( &dev->count_lock );
		return DRM_ERR(EBUSY);
d881 1
a881 1
	DRM_SPINUNLOCK( &dev->count_lock );
d883 1
a883 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_info_t *)data, sizeof(request) );
d897 1
a897 1
				if ( DRM_COPY_TO_USER( &to->count,
d900 1
a900 1
				     DRM_COPY_TO_USER( &to->size,
d903 1
a903 1
				     DRM_COPY_TO_USER( &to->low_mark,
d906 1
a906 1
				     DRM_COPY_TO_USER( &to->high_mark,
d909 1
a909 1
					return DRM_ERR(EFAULT);
d923 1
a923 1
	DRM_COPY_TO_USER_IOCTL( (drm_buf_info_t *)data, request, sizeof(request) );
d928 1
a928 1
int DRM(markbufs)( DRM_IOCTL_ARGS )
d930 1
a930 1
	DRM_DEVICE;
d936 1
a936 1
	if ( !dma ) return DRM_ERR(EINVAL);
d938 1
a938 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_desc_t *)data, sizeof(request) );
d944 1
a944 1
		return DRM_ERR(EINVAL);
d948 1
a948 1
		return DRM_ERR(EINVAL);
d950 1
a950 1
		return DRM_ERR(EINVAL);
d958 1
a958 1
int DRM(freebufs)( DRM_IOCTL_ARGS )
d960 1
a960 1
	DRM_DEVICE;
d967 1
a967 1
	if ( !dma ) return DRM_ERR(EINVAL);
d969 1
a969 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_free_t *)data, sizeof(request) );
d973 1
a973 1
		if ( DRM_COPY_FROM_USER( &idx,
d976 1
a976 1
			return DRM_ERR(EFAULT);
d980 1
a980 1
			return DRM_ERR(EINVAL);
d983 1
a983 1
		if ( buf->pid != DRM_CURRENTPID ) {
d985 2
a986 2
				   DRM_CURRENTPID, buf->pid );
			return DRM_ERR(EINVAL);
d994 1
a994 1
int DRM(mapbufs)( DRM_IOCTL_ARGS )
d996 1
a996 1
	DRM_DEVICE;
a1000 1
#ifdef __FreeBSD__
a1005 5
#endif /* __FreeBSD__ */
#ifdef __NetBSD__
	struct vnode *vn;
#endif /* __NetBSD__ */

d1009 1
a1009 1
	if ( !dma ) return DRM_ERR(EINVAL);
d1011 1
a1011 1
	DRM_SPINLOCK( &dev->count_lock );
d1013 2
a1014 2
		DRM_SPINUNLOCK( &dev->count_lock );
		return DRM_ERR(EBUSY);
d1017 1
a1017 3
	DRM_SPINUNLOCK( &dev->count_lock );

	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_map_t *)data, sizeof(request) );
d1019 1
a1019 4
#ifdef __NetBSD__
	if(!vfinddev(kdev, VCHR, &vn))
		return 0;	/* FIXME: Shouldn't this be EINVAL or something? */
#endif /* __NetBSD__ */
a1030 1
#ifdef __FreeBSD__
a1038 10
#elif defined(__NetBSD__)
			virtual = round_page((vaddr_t)vms->vm_daddr + MAXDSIZ);
			retcode = uvm_mmap(&vms->vm_map,
					   (vaddr_t *)&virtual,
					   round_page(map->size),
					   UVM_PROT_READ | UVM_PROT_WRITE,
					   UVM_PROT_ALL, MAP_SHARED,
					   &vn->v_uobj, map->offset,
					   p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
#endif /* __NetBSD__ */
a1039 1
#ifdef __FreeBSD__
a1047 10
#elif defined(__NetBSD__)
			virtual = round_page((vaddr_t)vms->vm_daddr + MAXDSIZ);
			retcode = uvm_mmap(&vms->vm_map,
					   (vaddr_t *)&virtual,
					   round_page(dma->byte_count),
					   UVM_PROT_READ | UVM_PROT_WRITE,
					   UVM_PROT_ALL, MAP_SHARED,
					   &vn->v_uobj, 0,
					   p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
#endif /* __NetBSD__ */
d1054 1
a1054 1
			if ( DRM_COPY_TO_USER( &request.list[i].idx,
d1060 1
a1060 1
			if ( DRM_COPY_TO_USER( &request.list[i].total,
d1066 1
a1066 1
			if ( DRM_COPY_TO_USER( &request.list[i].used,
d1073 1
a1073 1
			if ( DRM_COPY_TO_USER( &request.list[i].address,
d1086 1
a1086 1
	DRM_COPY_TO_USER_IOCTL( (drm_buf_map_t *)data, request, sizeof(request) );
d1088 1
a1088 1
	return DRM_ERR(retcode);
@


1.1.1.3
log
@That's what OpenBSD will, probably, ship as XF4 in 3.5
their last sync against XFree86 4.3-current has been
imported into our vendor branch, too
@
text
@a29 1
 *
d72 1
a72 2
	drm_map_t request;
	drm_local_map_t *map;
d78 1
a78 3
	DRM_COPY_FROM_USER_IOCTL( request, (drm_map_t *)data, sizeof(drm_map_t) );

	map = (drm_local_map_t *) DRM(alloc)( sizeof(*map), DRM_MEM_MAPS );
d82 2
a83 7
	map->offset = request.offset;
	map->size = request.size;
	map->type = request.type;
	map->flags = request.flags;
	map->mtrr   = -1;
	map->handle = 0;
	
d98 2
d104 3
a106 1
		if ( map->offset + map->size < map->offset ) {
d110 4
d117 21
a137 6
			int mtrr;
			     
			mtrr = DRM(mtrr_add)(map->offset, map->size,
			     DRM_MTRR_WC);
			if (mtrr == 0)
				map->mtrr = 1;
d140 1
a140 1
		DRM_IOREMAP(map, dev);
d145 1
a145 1
		DRM_DEBUG( "%lu %d %p\n",
a152 9
			/* Prevent a 2nd X Server from creating a 2nd lock */
			DRM_LOCK();
			if (dev->lock.hw_lock != NULL) {
				DRM_UNLOCK();
				DRM(free)(map->handle, map->size,
				    DRM_MEM_SAREA);
				DRM(free)(map, sizeof(*map), DRM_MEM_MAPS);
				return DRM_ERR(EBUSY);
			}
a153 1
			DRM_UNLOCK();
d158 3
d178 2
a179 2
	list = DRM(calloc)(1, sizeof(*list), DRM_MEM_MAPS);
	if (list == NULL) {
d183 1
d186 1
a186 1
	DRM_LOCK();
d188 1
a188 1
	DRM_UNLOCK();
d190 1
a190 6
	request.offset = map->offset;
	request.size = map->size;
	request.type = map->type;
	request.flags = map->flags;
	request.mtrr   = map->mtrr;
	request.handle = map->handle;
d192 2
a193 2
	if ( request.type != _DRM_SHM ) {
		request.handle = (void *)request.offset;
a194 3

	DRM_COPY_TO_USER_IOCTL( (drm_map_t *)data, request, sizeof(drm_map_t) );

d207 1
a207 1
	drm_local_map_t *map;
d209 1
d213 1
a213 1
	DRM_LOCK();
d216 2
a217 3
		if (map->handle == request.handle &&
		    map->flags & _DRM_REMOVABLE)
			break;
d220 5
a224 3
	/* No match found. */
	if (list == NULL) {
		DRM_UNLOCK();
d228 1
a228 1
	DRM_UNLOCK();
a229 1
	DRM(free)(list, sizeof(*list), DRM_MEM_MAPS);
d231 4
a234 3
	switch (map->type) {
	case _DRM_REGISTERS:
	case _DRM_FRAME_BUFFER:
d236 32
a267 6
		if (map->mtrr >= 0) {
			int __unused mtrr;
			
			mtrr = DRM(mtrr_del)(map->offset, map->size,
			    DRM_MTRR_WC);
			DRM_DEBUG("mtrr_del = %d\n", mtrr);
d269 1
a269 9
#endif
		DRM(ioremapfree)(map);
		break;
	case _DRM_SHM:
		DRM(free)(map->handle, map->size, DRM_MEM_SAREA);
		break;
	case _DRM_AGP:
	case _DRM_SCATTER_GATHER:
		break;
d271 1
a271 1
	DRM(free)(map, sizeof(*map), DRM_MEM_MAPS);
d278 1
a278 1
static void DRM(cleanup_buf_error)(drm_device_t *dev, drm_buf_entry_t *entry)
a281 1
#if __HAVE_PCI_DMA
d284 3
a286 4
			if (entry->seglist[i] != NULL)
				DRM(pci_free)(dev, entry->buf_size,
				    (void *)entry->seglist[i],
				    entry->seglist_bus[i]);
a291 2
		DRM(free)(entry->seglist_bus, entry->seg_count *
			  sizeof(*entry->seglist_bus), DRM_MEM_SEGS);
a294 1
#endif /* __HAVE_PCI_DMA */
d296 7
a302 4
   	if (entry->buf_count) {
	   	for (i = 0; i < entry->buf_count; i++) {
			DRM(free)(entry->buflist[i].dev_private,
			    entry->buflist[i].dev_priv_size, DRM_MEM_BUFS);
d309 4
d318 1
a318 1
static int DRM(addbufs_agp)(drm_device_t *dev, drm_buf_desc_t *request)
d320 1
d322 1
d337 6
a342 2
	count = request->count;
	order = DRM(order)(request->size);
d345 1
a345 1
	alignment  = (request->flags & _DRM_PAGE_ALIGN)
d351 1
a351 1
	agp_offset = dev->agp->base + request->agp_start;
d361 14
d376 11
d391 2
d413 1
d415 2
a416 1
		buf->filp    = NULL;
d419 3
a421 3
		buf->dev_private = DRM(calloc)(1, buf->dev_priv_size,
		    DRM_MEM_BUFS);
		if (buf->dev_private == NULL) {
d424 1
a424 2
			DRM(cleanup_buf_error)(dev, entry);
			return DRM_ERR(ENOMEM);
d426 8
d447 1
a447 1
	if (temp_buflist == NULL) {
d449 3
a451 1
		DRM(cleanup_buf_error)(dev, entry);
d466 12
a477 2
	request->count = entry->buf_count;
	request->size = size;
d481 1
d487 1
a487 1
static int DRM(addbufs_pci)(drm_device_t *dev, drm_buf_desc_t *request)
d489 1
d491 1
d498 1
a498 1
	vm_offset_t vaddr;
a506 1
	dma_addr_t bus_addr;
d508 6
a513 2
	count = request->count;
	order = DRM(order)(request->size);
d516 3
a518 2
	DRM_DEBUG( "count=%d, size=%d (%d), order=%d\n",
		   request->count, request->size, size, order );
d520 6
a525 1
	alignment = (request->flags & _DRM_PAGE_ALIGN)
d530 9
d540 5
d546 5
a550 6
	entry->buflist = DRM(alloc)(count * sizeof(*entry->buflist),
	    DRM_MEM_BUFS);
	entry->seglist = DRM(alloc)(count * sizeof(*entry->seglist),
	    DRM_MEM_SEGS);
	entry->seglist_bus = DRM(alloc)(count * sizeof(*entry->seglist_bus),
	    DRM_MEM_SEGS);
d552 8
a559 5
	/* Keep the original pagelist until we know all the allocations
	 * have succeeded
	 */
	temp_pagelist = DRM(alloc)((dma->page_count + (count << page_order)) *
	    sizeof(*dma->pagelist), DRM_MEM_PAGES);
d561 8
a568 8
	if (entry->buflist == NULL || entry->seglist == NULL || 
	    temp_pagelist == NULL) {
		DRM(free)(entry->buflist, count * sizeof(*entry->buflist),
		    DRM_MEM_BUFS);
		DRM(free)(entry->seglist, count * sizeof(*entry->seglist),
		    DRM_MEM_SEGS);
		DRM(free)(entry->seglist_bus, count *
		    sizeof(*entry->seglist_bus), DRM_MEM_SEGS);
d571 1
d573 16
a588 5
	bzero(entry->buflist, count * sizeof(*entry->buflist));
	bzero(entry->seglist, count * sizeof(*entry->seglist));
	
	memcpy(temp_pagelist, dma->pagelist, dma->page_count * 
	    sizeof(*dma->pagelist));
d590 1
d600 3
a602 15
		vaddr = (vm_offset_t) DRM(pci_alloc)(dev, size, alignment,
		    0xfffffffful, &bus_addr);
		if (vaddr == NULL) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			entry->seg_count = count;
			DRM(cleanup_buf_error)(dev, entry);
			DRM(free)(temp_pagelist, (dma->page_count +
			    (count << page_order)) * sizeof(*dma->pagelist),
			    DRM_MEM_PAGES);
			return DRM_ERR(ENOMEM);
		}
	
		entry->seglist_bus[entry->seg_count] = bus_addr;
		entry->seglist[entry->seg_count++] = vaddr;
d606 3
a608 3
				   (long)vaddr + PAGE_SIZE * i );
			temp_pagelist[dma->page_count + page_count++] = 
			    vaddr + PAGE_SIZE * i;
d619 1
a619 2
			buf->address = (void *)(vaddr + offset);
			buf->bus_address = bus_addr + offset;
d621 1
d623 8
a630 17
			buf->filp    = NULL;

			buf->dev_priv_size = sizeof(DRIVER_BUF_PRIV_T);
			buf->dev_private = DRM(alloc)(sizeof(DRIVER_BUF_PRIV_T),
			    DRM_MEM_BUFS);
			if (buf->dev_private == NULL) {
				/* Set count correctly so we free the proper amount. */
				entry->buf_count = count;
				entry->seg_count = count;
				DRM(cleanup_buf_error)(dev, entry);
				DRM(free)(temp_pagelist, (dma->page_count + 
				    (count << page_order)) *
				    sizeof(*dma->pagelist), DRM_MEM_PAGES );
				return DRM_ERR(ENOMEM);
			}
			bzero(buf->dev_private, buf->dev_priv_size);

d642 1
a642 1
	if (temp_buflist == NULL) {
d644 3
a646 4
		DRM(cleanup_buf_error)(dev, entry);
		DRM(free)(temp_pagelist, (dma->page_count + 
		    (count << page_order)) * sizeof(*dma->pagelist),
		    DRM_MEM_PAGES);
a654 7
	/* No allocations failed, so now we can replace the orginal pagelist
	 * with the new one.
	 */
	DRM(free)(dma->pagelist, dma->page_count * sizeof(*dma->pagelist),
	    DRM_MEM_PAGES);
	dma->pagelist = temp_pagelist;

d660 7
a666 2
	request->count = entry->buf_count;
	request->size = size;
d668 6
d680 1
a680 1
static int DRM(addbufs_sg)(drm_device_t *dev, drm_buf_desc_t *request)
d682 1
d684 1
d699 6
a704 2
	count = request->count;
	order = DRM(order)(request->size);
d707 1
a707 1
	alignment  = (request->flags & _DRM_PAGE_ALIGN)
d713 1
a713 1
	agp_offset = request->agp_start;
d723 13
d737 5
d743 11
a753 3
	entry->buflist = DRM(calloc)(1, count * sizeof(*entry->buflist),
	    DRM_MEM_BUFS);
	if (entry->buflist == NULL)
d755 2
d774 1
d776 2
a777 1
		buf->filp    = NULL;
d780 3
a782 3
		buf->dev_private = DRM(calloc)(1, buf->dev_priv_size,
		    DRM_MEM_BUFS);
		if (buf->dev_private == NULL) {
d785 3
a787 1
			DRM(cleanup_buf_error)(dev, entry);
d791 8
d814 1
a814 1
	if (temp_buflist == NULL) {
d816 3
a818 1
		DRM(cleanup_buf_error)(dev, entry);
d833 12
a844 2
	request->count = entry->buf_count;
	request->size = size;
d848 1
a854 1
	DRM_DEVICE;
a855 2
	int err;
	int order;
a858 19
	if (request.count < 0 || request.count > 4096)
		return DRM_ERR(EINVAL);

	order = DRM(order)(request.size);
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER)
		return DRM_ERR(EINVAL);

	DRM_SPINLOCK(&dev->dma_lock);
	/* No more allocations after first buffer-using ioctl. */
	if (dev->buf_use != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return DRM_ERR(EBUSY);
	}
	/* No more than one allocation per order */
	if (dev->dma->bufs[order].buf_count != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return DRM_ERR(ENOMEM);
	}

d861 1
a861 1
		err = DRM(addbufs_agp)(dev, &request);
d866 1
a866 1
		err = DRM(addbufs_sg)(dev, &request);
d870 1
a870 1
		err = DRM(addbufs_pci)(dev, &request);
d872 1
a872 1
		err = DRM_ERR(EINVAL);
a873 5
	DRM_SPINUNLOCK(&dev->dma_lock);

	DRM_COPY_TO_USER_IOCTL((drm_buf_desc_t *)data, request, sizeof(request));

	return err;
a882 1
	int retcode = 0;
d884 1
a884 1
	DRM_COPY_FROM_USER_IOCTL( request, (drm_buf_info_t *)data, sizeof(request) );
d886 5
a890 1
	DRM_SPINLOCK(&dev->dma_lock);
d892 3
a894 1
	DRM_SPINUNLOCK(&dev->dma_lock);
d905 16
a920 12
				drm_buf_desc_t from;

				from.count = dma->bufs[i].buf_count;
				from.size = dma->bufs[i].buf_size;
				from.low_mark = dma->bufs[i].freelist.low_mark;
				from.high_mark = dma->bufs[i].freelist.high_mark;

				if (DRM_COPY_TO_USER(&request.list[count], &from,
				    sizeof(drm_buf_desc_t)) != 0) {
					retcode = DRM_ERR(EFAULT);
					break;
				}
d936 1
a936 1
	return retcode;
d945 3
d953 4
a956 1
	
d958 1
a958 3
	order = DRM(order)(request.size);	
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER ||
	    request.low_mark < 0 || request.high_mark < 0) {
d960 1
a960 5
	}

	DRM_SPINLOCK(&dev->dma_lock);
	if (request.low_mark > dma->bufs[order].buf_count ||
	    request.high_mark > dma->bufs[order].buf_count) {
a961 1
	}
d963 2
a964 3
	dma->bufs[order].freelist.low_mark  = request.low_mark;
	dma->bufs[order].freelist.high_mark = request.high_mark;
	DRM_SPINUNLOCK(&dev->dma_lock);
d977 2
a978 1
	int retcode = 0;
a982 2
	
	DRM_SPINLOCK(&dev->dma_lock);
d984 4
a987 4
		if (DRM_COPY_FROM_USER(&idx, &request.list[i], sizeof(idx))) {
			retcode = DRM_ERR(EFAULT);
			break;
		}
d991 1
a991 2
			retcode = DRM_ERR(EINVAL);
			break;
d994 4
a997 5
		if ( buf->filp != filp ) {
			DRM_ERROR("Process %d freeing buffer not owned\n",
				   DRM_CURRENTPID);
			retcode = DRM_ERR(EINVAL);
			break;
a1000 1
	DRM_SPINUNLOCK(&dev->dma_lock);
d1002 1
a1002 1
	return retcode;
d1011 1
a1011 2
	vm_offset_t address;
	struct vmspace *vms;
d1013 5
a1017 3
	vm_ooffset_t foff;
	vm_size_t size;
	vm_offset_t vaddr;
a1020 2
	vm_size_t size;
	vaddr_t vaddr;
d1026 10
d1039 1
a1039 1
	if (!vfinddev(kdev, VCHR, &vn))
d1043 9
a1051 27
#if defined(__FreeBSD__) && __FreeBSD_version >= 500000
	vms = p->td_proc->p_vmspace;
#else
	vms = p->p_vmspace;
#endif

	DRM_SPINLOCK(&dev->dma_lock);
	dev->buf_use++;		/* Can't allocate more after this call */
	DRM_SPINUNLOCK(&dev->dma_lock);

	if (request.count < dma->buf_count)
		goto done;

	if ((__HAVE_AGP && (dma->flags & _DRM_DMA_USE_AGP)) ||
	    (__HAVE_SG && (dma->flags & _DRM_DMA_USE_SG))) {
		drm_local_map_t *map = DRIVER_AGP_BUFFERS_MAP(dev);

		if (map == NULL) {
			retcode = EINVAL;
			goto done;
		}
		size = round_page(map->size);
		foff = map->offset;
	} else {
		size = round_page(dma->byte_count),
		foff = 0;
	}
d1054 28
a1081 3
	vaddr = round_page((vm_offset_t)vms->vm_daddr + MAXDSIZ);
	retcode = vm_mmap(&vms->vm_map, &vaddr, size, PROT_READ | PROT_WRITE,
	    VM_PROT_ALL, MAP_SHARED, SLIST_FIRST(&kdev->si_hlist), foff );
d1083 8
a1090 4
	vaddr = round_page((vaddr_t)vms->vm_daddr + MAXDSIZ);
	retcode = uvm_mmap(&vms->vm_map, &vaddr, size,
	    UVM_PROT_READ | UVM_PROT_WRITE, UVM_PROT_ALL, MAP_SHARED,
	    &vn->v_uobj, foff, p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
a1091 10
	if (retcode)
		goto done;

	request.virtual = (void *)vaddr;

	for ( i = 0 ; i < dma->buf_count ; i++ ) {
		if (DRM_COPY_TO_USER(&request.list[i].idx,
		    &dma->buflist[i]->idx, sizeof(request.list[0].idx))) {
			retcode = EFAULT;
			goto done;
d1093 1
a1093 14
		if (DRM_COPY_TO_USER(&request.list[i].total,
		    &dma->buflist[i]->total, sizeof(request.list[0].total))) {
			retcode = EFAULT;
			goto done;
		}
		if (DRM_COPY_TO_USER(&request.list[i].used, &zero,
		    sizeof(zero))) {
			retcode = EFAULT;
			goto done;
		}
		address = vaddr + dma->buflist[i]->offset; /* *** */
		if (DRM_COPY_TO_USER(&request.list[i].address, &address,
		    sizeof(address))) {
			retcode = EFAULT;
d1095 28
a1124 1

d1130 1
a1130 1
	DRM_COPY_TO_USER_IOCTL((drm_buf_map_t *)data, request, sizeof(request));
d1136 1
@


